% Convention
% * British English
% * Footnotes are sentences, not only references in a footnote

\documentclass[a4paper, 12pt]{article}

\usepackage{amsthm, amsmath, amssymb, mathrsfs, multirow, url}
\usepackage{amsfonts} 
\usepackage[blocks,auth-sc-lg,affil-it]{authblk}
\renewcommand\Affilfont{\itshape\small}
\usepackage{caption}
\usepackage{csquotes}
\usepackage{subcaption}
\usepackage{marvosym}
\usepackage{makecell,booktabs,siunitx}
\usepackage{graphicx} 
\usepackage{ifthen} 
\usepackage[usenames]{color}
\usepackage{placeins}

\usepackage[margin=1in]{geometry}
\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{\scalebox{0.25}{
\includegraphics{resone-logo.png}}
\rhead{
{\small 
\url{https://www.researchers.one/article/2020-04-14}}
% \url{https://bit.ly/LML-pw}} % fill in URL here
}
}

\usepackage[colorlinks=true,urlcolor=blue,linkcolor=red,citecolor=blue]{hyperref}

% =====================================================================
% =========================  BibLateX w/ BibTeX   =====================
% =====================================================================
\usepackage[
arxiv=abs,																					% Link to arXiv abstract page
autocite=inline,																		% structur of \autocite{key}
autolang=hyphen,																		% active if langid is set
% autopunct=false,																	%
backend=bibtex,																		% Backend choice BibTeX8 case-sensitive sorting 
backref=true,																				% page cited in the paper
% bibencoding=latin1,
bibstyle=authoryear,																% style in bibliography
% bibstyle=draft,																		% style in bibliography
% bibwarn=true,
citestyle=authoryear-comp,													% style in the document
dashed=false,																				% always repeat author names in bibliography
% -- hyperref=true,																		%
% language=autobib,																		% bibliography language same as language of origin
% sortcites=true,																			% sort citations in the text
sortcites=false,																		% do not sort citations in the text
% sorting=nyt,																				% sorting in bibliography
maxbibnames=11,																			% limit authors in bibliography to 11
maxcitenames=2,																			% start to use et alii
% minbibnames=10																			% Angabe wie vieler Autoren nach Schnitt
mincitenames=1,																			% cut to X authors with et al.
url=true,doi=true,eprint=true,											% print url,doi,eprint
useprefix=true,																			% consider nobiliary particle von, van, de, etc.
]{biblatex}

\makeatletter
\def\blx@maxline{77}
\makeatother

\addbibresource{../LML_bibliography/bibliography}

% =====================================================================
% ===================   STYLE OF THE REFERENCES   =====================
% =====================================================================

% \newcommand*\person[1]{\textsc{#1}}									% prints author names as small caps
\renewcommand{\mkbibnamegiven}[1]{\person{#1}}
\renewcommand{\mkbibnamefamily}[1]{\person{#1}}
\renewcommand{\mkbibnameprefix}[1]{\person{#1}}
% \renewcommand{\mkbibnamesuffix}[1]{\textsc{#1}}

\setlength{\bibitemsep}{1.5\itemsep}			% space between 2 bibitems

\DeclareFieldFormat{pages}{#1}		% removes pagination (p./pp.) before page numbers

% \renewbibmacro{etal}{\textsc{et al.}}  % find the name for this macro

\renewbibmacro{in:}{}					% print no 'In:' before the <Journal Name>'

% \DefineBibliographyStrings{english}{%
%     backrefpage  = {cit. on p.}, 			% default: for single page number
%     backrefpage  = {}, 								% for single page number
%     backrefpages = {cit. on pp.} 			% default: for multiple page numbers
%     backrefpages = {} 								% for multiple page numbers
% }
% =====================================================================

\usepackage{xspace}

\newcommand{\bi}{\begin{itemize}}
\newcommand{\ei}{\end{itemize}}

\newcommand{\ave}[1]{\left\langle#1 \right\rangle}
\newcommand{\gamble}[1]{\textsc{#1}}
\newcommand{\person}[1]{\textsc{#1}\xspace}
\newcommand{\concept}[1]{{\sc#1}}
\newcommand{\elabel}[1]{\label{eq:#1}}
\newcommand{\eref}[1]{(Eq.~\ref{eq:#1})}
\newcommand{\ceref}[2]{(\ref{eq:#1}#2)}
\newcommand{\Eref}[1]{Equation~(\ref{eq:#1})}
\newcommand{\flabel}[1]{\label{fig:#1}}
\newcommand{\fref}[1]{Fig.~\ref{fig:#1}}
\newcommand{\Fref}[1]{Figure~\ref{fig:#1}}
\newcommand{\tlabel}[1]{\label{tab:#1}}
\newcommand{\tref}[1]{Tab.~\ref{tab:#1}}
\newcommand{\Tref}[1]{Table~\ref{tab:#1}}
\newcommand{\seclabel}[1]{\label{sec:#1}}
\newcommand{\secref}[1]{Sec.~\ref{sec:#1}}
\newcommand{\Secref}[1]{Section~\ref{sec:#1}}
\newcommand{\appref}[1]{App.~\ref{sec:#1}}
\newcommand{\Appref}[1]{Appendix~\ref{sec:#1}}
\newcommand{\nmax}{\ensuremath{n_{\text{max}}}\xspace}
\newcommand{\gens}{\ensuremath{g_\text{e}}}
\newcommand{\gtime}{\bar{g}}
\newcommand{\gi}{g^{(i)}}
\newcommand{\Ito}{It\^{o}\xspace}

\newcommand{\etal}{\textit{et al.}\xspace}
\newcommand{\apriori}{\textit{a priori}\xspace}
\newcommand{\ia}{\textit{i.a.}\xspace}
\newcommand{\ie}{\textit{i.e.}\xspace}
\newcommand{\etc}{\textit{etc.}\xspace}
\newcommand{\eg}{\textit{e.g.}\xspace}
\newcommand{\Eg}{\textit{E.g.}\xspace}
\newcommand{\cf}{\textit{cf.}\xspace}
\newcommand{\vs}{\textit{vs.}\xspace}
\newcommand{\vv}{\textit{v.v.}\xspace }
\newcommand{\gensst}{g_{\text{est}}}
\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}
\newcommand{\bea}{\begin{eqnarray}}
\newcommand{\eea}{\end{eqnarray}}
\newcommand{\D}{\Delta}
\newcommand{\yn}[1]{{y^{(#1)}}}
\newcommand{\xin}[1]{{\xi^{(#1)}}}
\newcommand{\Wn}[1]{{W^{(#1)}}}
\newcommand{\mun}[1]{{\mu^{(#1)}}}
\newcommand{\sigman}[1]{{\sigma^{(#1)}}}
\newcommand{\sigmac}{{\sigma_\rho}}
\newcommand{\err}[1]{\varepsilon\left[#1\right]}
\newcommand{\phat}{\hat{p}}
\newcommand{\nhat}{\hat{n}}

\newcommand{\ND}{\mathcal{N}} % Normal Distribution
\newcommand{\sigmat}{\tilde{\sigma}}
%\newcommand{\var}[1]{\text{var}(#1)}
\newcommand{\MK}[1]{\textcolor{red}{\textit{***MK: #1 MK***}}}
\newcommand{\OP}[1]{{\it ***OP: #1 OP***}}
\newcommand{\YB}[1]{{\it ***YB: #1 YB***}}
\renewcommand{\AA}[1]{{\it ***AA: #1 AA***}}

\title{What are we weighting for? \\
{\normalsize A mechanistic model for probability weighting\thanks{The latest version of the manuscript is available at \href{https://bit.ly/lml-pw}{bit.ly/lml-pw}. The code to generate the figures and simulations is available at \href{https://bit.ly/lml-pw-code}{bit.ly/lml-pw-code}.}}}

% Authors with Email addresses as footnotes
\author[1,2]{Ole Peters\thanks{\texttt{\href{mailto:o.peters@lml.org.uk}{~\Letter~o.peters@lml.org.uk}}}}
\author[1]{Alexander Adamou\thanks{\texttt{\href{mailto:a.adamou@lml.org.uk}{~\Letter~a.adamou@lml.org.uk}}}}
\author[1,3,4]{Mark Kirstein\thanks{\texttt{\href{mailto:m.kirstein@lml.org.uk}{~\Letter~m.kirstein@lml.org.uk}}}}
\author[1,5]{Yonatan Berman\thanks{\texttt{\href{mailto:y.berman@lml.org.uk}{~\Letter~y.berman@lml.org.uk}}}}
% \affil[1]{London Mathematical Laboratory, 8 Margravine Gardens, London W6 8RH, UK}
\affil[1]{London Mathematical Laboratory, London, UK} % short version

% \affil[2]{Santa Fe Institute, 1399 Hyde Park Road, Santa Fe, 87501 NM, USA}
\affil[2]{Santa Fe Institute, NM, USA} % short version
% \affil[3]{Max-Planck-Institute for Mathematics in the Sciences, Inselstraße 22, D-04103 Leipzig, Germany}
\affil[3]{Max-Planck-Institute for Mathematics in the Sciences, Leipzig, Germany} % short version
% \affil[4]{Institute of Mathematics, Augustusplatz 10, D-04109 Leipzig University}
\affil[4]{Institute of Mathematics, Leipzig University, Leipzig, Germany} % short version
% \affil[5]{The Graduate Center and Stone Center on Socio-Economic Inequality, City University of New York}
\affil[5]{The Graduate Center and Stone Center on Socio-Economic Inequality, City University of New York, NY, USA}

\date{\today}

\begin{document}
\begin{titlepage}
	\maketitle
\thispagestyle{fancy}

\begin{abstract}
\noindent 
Behavioural economics provides labels for patterns in human economic behaviour. Probability weighting is one such label. It expresses a mismatch between probabilities used in a formal model of a decision (\ie model parameters) and probabilities inferred from real people's decisions (the same parameters estimated empirically). The inferred probabilities are called ``decision weights.'' It is considered a robust experimental finding that decision weights are higher than probabilities for rare events, and (necessarily, through normalisation) lower than probabilities for common events. Typically this is presented as a cognitive bias, \ie an error of judgement by the person.
Here we point out that the same observation can be described differently: broadly speaking, probability weighting means that a decision maker has greater uncertainty about the world than the observer.
We offer a plausible mechanism whereby such differences in uncertainty arise naturally: when a decision maker must estimate probabilities as frequencies in a time series while the observer knows them \textit{a priori}. This suggests an alternative presentation of probability weighting as a principled response by a decision maker to uncertainties unaccounted for in an observer's model.
\vspace{1em}

\noindent\textsf{\textbf{Keywords}} ~ Decision Theory, Prospect Theory, Probability Weighting, Ergodicity Economics
\vspace{.5em}

\noindent\textsf{\textbf{JEL Codes}} ~
\href{https://www.aeaweb.org/econlit/jelCodes.php?view=jel#C}{%
C61		% Optimization Techniques • Programming Models • Dynamic Analysis
$\cdot$
}%
\href{https://www.aeaweb.org/econlit/jelCodes.php?view=jel#D}{%
D01 	% Microeconomic Behaviour: Underlying Principles
$\cdot$
D81 	% Criteria for decision making under Risk and Uncertainty
}
\end{abstract}
\end{titlepage}
 
\setcounter{page}{2}		% titlepage=page 1, continue with page 2

\section{Introduction}
\textit{Probability weighting} is a concept that originated in prospect theory \parencite{KahnemanTversky1979,TverskyKahneman1992}. It is one way to conceptualise a pattern in human behaviour of caution with respect to formal models.
This is best explained by a thought experiment, in which  
\bi
	\item a \textit{disinterested observer} (DO), such as an experimenter, tells
	\item a \textit{decision maker} (DM)
\ei
that an event occurs with some probability. The DO observes the DM's behaviour (\eg gambling on the event) and finds it consistent with a behavioural model (\eg expected-utility optimization) in which the DM uses a probability that differs systematically from what the DO has declared. The apparent probabilities, inferred from the DM's decisions, are called ``\textit{decision weights}.'' We will adopt this nomenclature here.
% 
\bi
	\item By ``\textit{probabilities,}'' expressed as probability density functions (PDFs) and denoted $p(x)$, we will mean the numbers specified by a DO.
	\item By ``\textit{decision weights,}'' also expressed as PDFs and denoted $w(x)$, we will mean the numbers that best describe the behaviour of a DM in the DO's behavioural model.\footnote{In the literature, decision weights are not always normalised, but for simplicity we will work with normalised decision weights. Mathematically speaking, they are therefore proper probabilities even though we don't call them that. Our results are unaffected because normalising just means dividing by a constant (the sum or integral of the non-normalised decision weights).}
\ei
Here, $x$ is a realisation of a random variable, $X$. For example, $X$ might be the payout of a gamble which the DM is invited to accept or decline.

Different behavioural models may result in different inferred decision weights. Our focus is not on how these weights are inferred, but on the robust observation that decision weights, $w(x)$ (used by DMs), are higher than probabilities, $p(x)$ (declared by DOs), for extreme events, \ie when $p(x)$ is small. Thus, we do not consider any specific behavioural model: our aim is to find a general mechanistic explanation to probability weighting.

Probability weighting is often summarised visually by comparing 
\bi
\item cumulative density functions (CDFs) for probabilities, denoted 
\be
F_p(x)=\int_{-\infty}^x p(s) ds~,
\ee
\item and CDFs for decision weights, denoted
\be
F_w(x)=\int_{-\infty}^x w(s) ds~.
\ee
\ei
In \fref{TK1992} we reproduce the first such visual summary from \textcite[310]{TverskyKahneman1992}.
\begin{figure}[!htb]
\centering
\includegraphics[width=0.5\textwidth]{./figs/TK1992.pdf}
\caption{\textbf{Empirical phenomenon of probability weighting.} Cumulative decision weights $F_w$ (used by decision makers) versus cumulative probabilities $F_p$ (used by disinterested observers), as reported by \textcite[p.~310, Fig. 1, relabelled axes]{TverskyKahneman1992}. The figure is to be read as follows: pick a point along the horizontal axis (the cumulative probability $F_p$ used by a DO) and look up the corresponding value on the vertical axis of the dotted inverse-S curve (the cumulative decision weight $F_w$ used by a DM). Low cumulative probabilities (left) are exceeded by their corresponding cumulative decision weights, and for high cumulative probabilities it's the other way around. It's the inverse-S shape of the curve that indicates this qualitative relationship.}
\flabel{TK1992}
\end{figure}

Plotting $F_w$ as a function of $F_p$ generates a curve, whose generic shape we shall call the ``inverse-S''. The inverse-S is the main observational finding in probability weighting: it sits above the diagonal $F_w=F_p$ for events of low cumulative probability (such that $F_w>F_p$ for these events) and below the diagonal for events of high cumulative probability (such that $F_w<F_p$).

As a final piece of nomenclature, we will use the terms \textit{location}, \textit{scale}, and \textit{shape} when discussing probability distributions. Consider a standard normal distribution $\ND(0,1)$, whose parameters indicate location 0 and squared scale 1. (For a Gaussian, the location is the mean and scale is the standard deviation.) For a general random variable $X$, with arbitrary location parameter $\mu_X$ and scale parameter $\sigma_X$, the following transformation produces a standardised random variable with identically-shaped distribution, but with location 0 and scale 1:
\be
Z = \frac{X-\overbrace{\mu_X \mathstrut}^{\text{location}}}{\underbrace{\sigma_X\mathstrut}_{\text{scale}}}~.
\elabel{SN}
\ee
Thus the PDF of $Z$, $p(z)$, is a density function with location $\mu_Z=0$ and scale $\sigma_Z=1$. In a graph of a distribution, a change of location shifts the curve to the left or right, and a change in scale shrinks or blows up the width of its features. Neither operation changes the \textit{shape} of the distribution: two distributions have the same \textit{shape} if they can be made to coincide through a linear transformation of the form \eref{SN}.

%\newpage
\subsection{Related literature}

%  Tell our story
%\textit{Many researchers believe probability weighting to be a stable pattern of human behavior. This has been disputed on grounds of poor experimental design and statistical insignificance. Our impression: there is something to this story but the terms of the debate are not well defined, and a deeper understanding is needed to predict with more confidence what effect will be observed in a given situation.
%}
%
%We begin with a brief review of the developments in decision theory that led up to cumulative prospect theory in \secref{OptimandHistoryReview} in order to place our contribution within the context of the existing literature on probability weighting. 
%Before we add our own critique, we review in \secref{CPTCritiques} existing critiques of CPT and PW.
%\MK{In \secref{OtherCritiques} we focus upon *other* critiques. Slide-in might be necessary if we ``cycle'' the EE idea in the \secref{EEview} and dispensible if we dont'.}
%We conclude the section in \secref{EEview}, in which we locate this contribution within the overall research programme of ergodicity economics (EE) from which this paper originated from. Our critique is aimed at a conceptual element that entered decision theory much earlier than the elements that are more typically critiqued and surveyed in \secref{OptimandHistoryReview} and \secref{CPTCritiques}. Specifically, EE rejects the implicit assumption that the expected value of some quantity -- a linear average over the ensemble of possible future values -- is relevant to a DM. EE studies physically relevant quantities, which forces us to consider carefully the physical situation of an individual. This obviates the needs for psychological explanations: representing the information available to the DM is enough to make testable predictions of probability weighting.
%
%% (We could think of something attractive here, called a promise: what will the reader be able to do/understand after reading our manuscript? A: that psychology, which is problematic because it's subjective and makes no predictions, can be thrown out in many cases)
%
%\subsubsection{Brief review of the story of the optimand up to CPT} \seclabel{OptimandHistoryReview}

%EVT
\person{Pascal} and \person{Fermat}, in their famous exchange of letters, and later Huygens, laid the foundations of probability theory in the 17th century as a tool to analyse gambles. This has led to expected value theory (EVT), which formalised rational choice as an optimisation problem. In EVT, an expectation value of some quantity, \eg the gamble payoff, was thought to be \textit{the} suitable optimand.

%EUT
Lack of predictive power and misalignment with empirical data led to an abundance of paradoxes and puzzles. This has led, in turn, to new decision theories. These theories only altered the quantities contained in the optimand, \ie probabilities and absolute monetary changes. In doing so, more degrees of freedom have been added, leaving the expectation value as the optimand intact. For instance, in expected utility theory (EUT) a new degree of freedom is introduced with the transformation of the absolute monetary changes. The axiomatisation of EUT by \textcite{vonNeumannMorgenstern1944} and \textcite{Savage1954} lend further credence to expectation values as optimands and EUT as a normative decision theory.

%CPT
As a reaction to empirical violations of EUT, \textcite{KahnemanTversky1979} proposed first prospect theory (PT) and later cumulative prospect theory (CPT) \parencite{TverskyKahneman1992} as descriptive decision theories. CPT uses nonlinear transformations of absolute payoffs (value function) and probabilities in decision problems. The transformation of probabilities is known as probability weighting and is the focus of this paper. Following the consensus interpretation, probability weighting is a systematic maladaptive psychological effect of the overestimation of rare events and underestimation of common events.\footnote{An early example of this interpretation can be found in \textcite[Fig. 1, p. 188]{PrestonBaratta1948}.}

%\subsubsection{Critiques of CPT} \seclabel{CPTCritiques}
Many studies question the validity of the CPT explanation to probability weighting. Some critiques concern the origin of the additional degrees of freedom. Among others \textcite{StewartETAL2015} find no stable mappings from objective probabilities to subjective equivalents in individual DMs. They find that the specific mappings depend on the choice environment and are influenced to a large extent by the sampling method.
% They do not question the psychological objective-to-subjective mappings of CPT in principle, but propose even more flexible models with context-dependent mappings for the involved transformations of utility and probability weighting. However, they conclude that the sampling involved in the decision making has a crucial effect (p. 701).

% Description-Experience Gap
Research under the heading of the \textit{description-experience gap} further demonstrated the importance of how DMs are informed of the model probabilities in a decision problem. In \textit{decisions-from-description} DMs are essentially told what the probabilities are by DOs. In \textit{decisions-from-experience} DMs estimate model parameters from experiential sampling over time and learn through sequential feedback. \textcite{HertwigETAL2004} find stronger overweighting of rare events in decisions-from-description than in decisions-from-experience. Experience-based experiments even commonly find underweighting of rare events \parencite{UngemachETAL2009}.\footnote{It was realised that formerly dominant description-based experiments suffer from lessened external validity. Real-world decisions rarely provide clearly defined probabilities, see also \secref{Reasons_for}. Experience and learning are more important. Therefore, roughly since the year 2000, experience-based experiments prevail or studies contrast sampling modes like \textcite{HertwigETAL2004,HertwigErev2009,ErevETAL2010}. Furthermore, in animal studies only the decisions-from-experience sampling paradigm is feasible.}
% all possible PW curves are reported
%Such conflicting evidence is also found on a wide level in a meta-analytic review by \textcite[Tab. 9]{WulffETAL2018}, who report all possible forms of probability weighting curves. This clearly weakens any consensus about a typical inverse-S shape of the probability weighting curve.

% Statistical explanations rather than psychological explanation of PW
Other studies emphasise statistical explanations rather than psychological explanations of probability weighting. For instance, \textcite{Martins2006} explains probability weighting as a heuristic that approximates an evolutionary optimal Bayesian solution to the inference problem of the DM. Similar to our results but in a Bayesian setting, \textcite{SeoETAL2019} find that optimal beliefs of the DM deviate from the true prior in a sequential learning problem. Thus it is optimal to overweight small priors and underweight large priors. \textcite{FoxHadar2006,UngemachETAL2009}, on the other hand, report that probability weighting persists even after controlling for such effects, suggesting that a psychological component may still exist.

% our critique and should go to the end
Here we offer a statistical explanation of probability weighting that applies to any situation where differences between models of uncertainty arise. The proposed mechanism may generate a mismatch between probabilities in a normative model (used by a DO) and decision weights (used by a DM) and thus every possible shape of the probability weighting curve.
%
An inverse-S curve emerges when the DM cautiously incorporates more uncertainty into his model than a DO does in his model. Our statistical explanation suggests that this behaviour is a plausible strategy in decision making under uncertainty.
%
%If solely judged by the capability to fit the reported data in \fref{curvefit}, our proposed mechanism suggests that a statistical explanation of probability weighting is sufficient.
% 
%Furthermore, this paper helps to showcase how the purely psychological explanations might mislead, as they do not make clear testable predictions.
%
In contrast to other studies on the subject, we assume the DM corrects his estimate for the sampling error, whereas existing studies stop at the identification of the sampling error, and thus believe the DM uses an underweighted and biased estimate of rare events.

%\subsubsection{Then all the other critiques (pick an order that makes sense)}\seclabel{OtherCritiques}
%\MK{If we find it useful to discuss further material we could do it here, I think esp. about  \textcite{PriceJones2020} \ldots}

%\subsubsection{Ergodicity economics} \seclabel{EEview}
% "cycling" the same EE idea repeatedly from slightly different angle
% angle here: no behavioural model

The DM's response to the experienced outcomes over is fundamental in ergodicity economics \parencite{Peters2019}. The relevant insight from ergodicity economics that drives this paper is to ask what a DM can reasonably infer (specifically about the frequencies of different outcomes) from the available information, \eg a statement by a DO with no corroborating evidence or a finite sample of observed outcomes. Ergodicity economics pursues another path right from the outset of decision theory \parencites{Bernoulli1738}[see also][]{Peters2019} in identifying ergodic observables, like the large-time limit of the growth rate of wealth, that captures the experience of individual DMs better than expected quantities do \parencite{Peters2011a,Peters2011b,Peters2019b}. Conventional decision theories, including CPT, maintain ensemble-average quantities as their optimands, and justify that by psychological effects. Ergodicity economics does not refute the psychological effects, but rather postulates that these are secondary to growth-optimal behaviour, which is found to describe well many deviations from standard EUT \parencite{AdamouETAL2019,MederETAL2019}. Our mechanism explains probability weighting as a statistical effect that relies on the distinction between the two models of uncertainty held by the DO and the DM. Importantly, this is independent from a particular behavioural model -- neither EVT, EUT, CPT nor growth-optimal preferences. The phenomenon labelled probability weighting is a result of a generic inference problem, in which the DM is simply confronted with a different risk than the DO.

%\MK{ToDos: straighten abbreviations EVT, EUT, CPT etc and its first use \ldots}

%\newpage

\section{Probability weighting as a difference between models} \seclabel{ModelDiff}
Behavioural economics interprets \fref{TK1992} as evidence for a cognitive bias of the DM, an error of judgement. We will keep a neutral stance. We don't assume the DO to know ``the truth'' -- he has a model of the world. Nor do we assume the DM to know ``the truth'' -- he has another model of the world. From our perspective \fref{TK1992} merely shows that the two models differ. It says nothing about who is right or wrong.

\subsection{The inverse-S curve\seclabel{The_inverse}}
\subsubsection{Tversky and Kahneman}
\textcite{TverskyKahneman1992} chose to fit the empirical data in \fref{TK1992} with the following function,
%
\be
\elabel{correspondence}
\tilde{F}^{TK}_w\left(F_p; \gamma\right) = \left(F_p\right)^\gamma \frac{1}{\left[\left(F_p\right)^\gamma+\left(1-F_p\right)^\gamma\right]^{1/\gamma}} ~,
\ee
%
which maps from one CDF, $F_p$, to another, $F_w$. We note that no mechanistic motivation was given for fitting this specific family of CDF mappings, parameterised by $\gamma$. The motivation is purely phenomenological: with $\gamma<1$, this function can be made to fit to the data reasonably well.

The function $\tilde{F}^{TK}_w\left(F_p; \gamma \right)$ has one free parameter, $\gamma$. For $\gamma=1$ it is the identity, and the CDFs coincide, $\tilde{F}^{TK}_w\left(F_p\right)=F_p$. Further, $\tilde{F}^{TK}_w$ has the following property: any curvature moves the intersection with the diagonal away from the mid-point $1/2$. This means if the function is used to fit an inverse-S (where $\gamma<1$), the fitting procedure itself introduces a shift of the intersection to the left. We consider the key observation to be the inverse-S shape, whereas the shift to the left may be an artefact of the function chosen for the fit.

\subsubsection{Scale, location, and the inverse-S}
We now make explicit how the robust qualitative observation of the inverse-S shape in \fref{TK1992} emerges when the DM uses a larger scale in his model of the world than the DO.

We illustrate this with a Gaussian distribution.
Let's assume that a DO models an observable $x$ -- which will often be a future change in wealth -- as a Gaussian with location $\mu$ and variance $\sigma^2$. And let's further assume that a DM models the same observable as a Gaussian with the same location, $\mu$, but with a greater scale, so that the variance is $(\alpha\sigma)^2$. The DM simply assumes a broader range -- $\alpha$ times greater -- of plausible values, left panel of \fref{mapping_pdfs}.
\begin{figure}[!htb]
\centering
\includegraphics[width=\textwidth]{./figs/mapping_pdfs.pdf}
\caption{\textbf{Mapping PDFs.} Left: probability PDF (red), estimated by a DO; and decision-weight PDF (blue), estimated by a DM. The DO models $x$ with a best estimate for the scale (standard deviation) and assumes the true frequency distribution is the red line. The DM models $x$ with a greater scale (here 2 times greater, $\alpha=2$), and assumes the true frequency distribution is the blue line. Comparing the two curves, the DM appears to the DO as someone who over-estimates probabilities of low-probability events and underestimates probabilities of high-probability events, indicated by vertical arrows.
Right: the difference between decision weights and probabilities can also be expressed by directly plotting, for any value of $x$, the decision weight \vs the probability observed at $x$. This corresponds to a non-linear distortion of the horizontal axis. The arrows on the left correspond to the same $x$-values as on the right. They therefore start and end at identical vertical positions as on the left. Because of the non-linear distortion of the horizontal axis, they are shifted to different locations horizontally.}
\flabel{mapping_pdfs}
\end{figure}

If the DM uses a greater scale in his model, then decision weights are higher than probabilities for low-probability events, and (because of normalisation) lower than probabilities for high-probability events. We can express this by plotting, for any value of $x$, the decision weight \vs the probability observed at $x$, right panel of \fref{mapping_pdfs}.

In the Gaussian case we can write the distributions explicitly as
\be
w(x)=\frac{1}{\sqrt{2\pi (\alpha \sigma)^2}}\exp\left[\frac{-(x -\mu )^2}{2 (\alpha \sigma)^2}\right]~,
\elabel{DecisionW}
\ee
and
\be
p(x)=\frac{1}{\sqrt{2\pi \sigma^2}}\exp\left[\frac{-(x -\mu )^2}{2 \sigma^2}\right] ~.
\elabel{p}
\ee
Eliminating $(x-\mu)^2$ from \eref{DecisionW} and \eref{p} yields the following expression for decision weight as a function of probability:
\be
w(p)= p^{\frac{1}{\alpha^2}} \frac{\left(2\pi\sigma^2\right)^{\frac{1-\alpha^2}{2\alpha^2}}}{\alpha} ~.
\elabel{w_of_p}
\ee
We plot this in the right panel of \fref{mapping_pdfs}. As a sanity check, consider the shape of the $w(p)$ (blue curve, right panel \fref{mapping_pdfs}): for a given value of $\alpha$, it is just a power law in $p$ with some pre-factor that ensures normalization. If $\alpha>1$ it means that the DM uses a greater standard deviation than the DO. In this case, the exponent of $p$ satisfies $\frac{1}{\alpha^2}<1$, and the blue curve is above the diagonal for small densities and below it for large densities. If $\alpha<1$ it means that the DM uses a smaller standard deviation than the DO. This can occur, for example, if the DM is an insider and has more information than provided to him by the DO. Instead of an inverse-S shape, this can lead to an S-shaped curve (see \Appref{Scurve}).

Alternatively, we can express the difference between models by plotting the CDFs $F_w$ and $F_p$. We do this in \fref{mapping_cdfs}, where the inverse-S emerges purely from the DM's greater assumed scale, $\alpha \sigma$.
\begin{figure}[!htb]
\centering
\includegraphics[width=\textwidth]{./figs/mapping_cdfs.pdf}
\caption{\textbf{Mapping CDFs.}
Left: The DO assumes the observable $X$ follows Gaussian distribution $X \sim \ND(0,1)$, which results in the red CDF of the standard normal, $F_p(x) = \Phi_{0,1}(x)$. The DM is more cautious, in his model the same observable $X$ follows a wider Gaussian distribution, $X \sim \ND(0,4)$ depicted by $F_w(x)$ (blue).
%
Following the vertical arrows (left to right), we see that for low values of the event probability $x$ the DM's CDF is larger than the DO's CDF, $F_p(x) < F_w(x)$; the curves coincide at 0.5 because no difference in location is assumed; necessarily for large values of the event probability $x$ the DM's CDF must be lower than the DO's.
Right: the same CDFs as on the left but now plotted not against $x$ but against the CDF $F_p$. Trivially, the CDF $F_p$ plotted against itself is the diagonal; the CDF $F_w$ now displays the generic inverse-S shape known from prospect theory. The arrows start and end at the same vertical values as on the left.
}
\flabel{mapping_cdfs}
\end{figure}

\FloatBarrier

\subsection{Different scales and locations\seclabel{A_mismatch}}

In \fref{Gauss_scale_location_both_KT} we explore what happens if both the scales and the locations of the DO's and DM's models differ. Visually, this produces an excellent fit to empirical data, to which we will return in \secref{Fitting_the}.
\begin{figure}[!htb]
\centering
\includegraphics[width=1.0\textwidth]{./figs/Gauss_scale_location_both_KT.pdf}
\caption{\textbf{CDF maps for Gaussian distributions.} Top left: Difference in location. DO assumes location 0, scale 1; DM assumes location 0.23 (bigger than DO), scale 1. Top right: Difference in scale. DO assumes location 0, scale 1; DM assumes location 0, scale 1.64 (broader than DO). Bottom left: Differences in scale and location. DO assumes location 0, scale 1; DM assumes location 0.23 (bigger than DO), scale 1.64 (broader than DO). Bottom right: Fit to observations reported by \textcite{TverskyKahneman1992}. This is \eref{correspondence} with $\gamma=0.65$. Note the similarity to bottom left.}
\flabel{Gauss_scale_location_both_KT}
\end{figure}
A difference in assumed scales and locations, for simple Gaussian distributions, is sufficient to reproduce the observations. This suggests a different nomenclature and a conceptual clarification. The inverse-S curve does not mean that ``probabilities are re-weighted.'' It means only that experimenters and their subjects have different views about appropriate models of, and responses to, a situation.

\FloatBarrier

\subsection{Different shapes\seclabel{Different_shapes}}
Numerically, our procedure can be applied to arbitrary distributions:
\begin{enumerate}
\item
construct a list of values for the CDF assumed by the DO, $F_p(x)$.
\item
construct a list of values for the CDF assumed by the DM, $F_w(x)$.
\item
plot $F_w(x)$ \vs $F_p(x)$.
\end{enumerate}
Of course, the DM could even assume a distribution whose shape differs from that of the DO's distribution.
The inverse-S arises whenever a DM assumes a greater scale for a unimodal distribution.
To illustrate the generality of the procedure, in \fref{Student-t} we carry it out for Student's (power-law tailed) $t$-distributions (which we refer to as $t$-distributions), where DO and DM use different shape parameters and different locations
\footnote{
The PDF of the $t$-distribution is
%
\be
p\left(x\right) = \frac{\Gamma\left(\frac{\nu+1}{2}\right)} {\Gamma\left(\frac{\nu}{2}\right)\sqrt{\pi\nu}\sigma} \left(1+\frac{1}{\nu}\left(\frac{x-\mu}{\sigma}\right)^2 \right)^{-\frac{\nu+1}{2}}\,,
\ee
%
where $\nu$ is the shape parameter, $\sigma$ is the scale parameter, and $\mu$ is the location parameter. The corresponding CDF is
%
\be
F\left(x\right) =
\begin{cases}
1 - \frac{1}{2} I_{\frac{\nu}{\left(\frac{x-\mu}{\sigma}\right)^2 + \nu}}\left(\frac{\nu}{2},\frac{1}{2}\right) &\text{ if } x-\mu \geq 0\,;\\
\frac{1}{2} I_{\frac{\nu}{\left(\frac{x-\mu}{\sigma}\right)^2 + \nu}}\left(\frac{\nu}{2},\frac{1}{2}\right) &\text{ if } x-\mu < 0\,,
\end{cases}
\ee
%
where $I_x\left(a,b\right)$ is the incomplete beta function.

In the limit $\nu \rightarrow \infty$, the $t$-distribution converges to a Gaussian with location $\mu$ and scale $\sigma$. We assume by default that $\sigma = 1$, so the $t$-distribution is effectively characterised by two parameters: shape ($\nu$) and location ($\mu$).
}
The result is qualitatively similar to the bottom right panel of \fref{mapping_cdfs}, corresponding to \eref{correspondence}.
\begin{figure}[!htb]
\centering
\includegraphics[width=0.5\textwidth]{./figs/Student-t.pdf}
\caption{\textbf{Probability weighting for $t$-distributions.} The DM uses different shape and location parameters (1 and 0.35 respectively) from those of the DO (3 and 0.2).}
\flabel{Student-t}
\end{figure}

\section{Reasons for different models\seclabel{Reasons_for}}
Probability weighting is usually interpreted as a cognitive bias that leads to errors of judgement and poor decisions by DMs.\footnote{Indeed, its originators presented it as such.
Introducing prospect theory, \textcite[277]{KahnemanTversky1979} wrote ``we are compelled to assume [\dots] that decision weights do not coincide with stated probabilities. These departures from expected utility theory must lead to normatively unacceptable consequences''.
They classified prospect theory as descriptive rather than normative, \ie as relating to actual rather than optimal behaviour \parencite[p. S252]{TverskyKahneman1986}.
Put simply, prospect theory aims to model systematic errors in human decision making, arising (in part) from inappropriate psychological adjustments of known probabilities.}
We caution against this interpretation.
At least we should keep in mind that it is unclear who suffers from the bias: experimenter or test subject (or neither or both)?
We are by no means the first to raise this question.
Commenting on another so-called cognitive bias regarding probabilities, the representativeness fallacy, \textcite{Cohen1979a} asked: ``Whose is the fallacy?''

Whatever the answer, two observations are robust and interesting: first, disagreement is common; and, second, the disagreement tends to go in the same direction, with DMs assuming a greater range of plausible outcomes than DOs.

An explanation for the first observation is that probability is a slippery concept and the word is used to mean different things. This suggests that phrasing information about probabilities concretely should reduce disagreement between DO and DM. For example, the statement ``10 out of 100 people have this disease'' conveys more, and more precise, information than ``the probability of having this disease is 0.1.'' Specifically, it tells us that a sample of people has been observed and what the size of the sample is.

Furthermore, \textcite{Gigerenzer2018} argues that statements involving integer counts, or what he calls natural frequencies,  (``10 out of 100'') are more readily understood by people than statements involving fractional probabilities (``0.1'').

The second observation may be explained as follows. A DO often has control over, and essentially perfect knowledge of, the decision problem he poses. A DM does not have such knowledge, and this ignorance will often translate into additional assumed uncertainty. For example, the DO may know the true probabilities of some gamble in an experiment, while the DM may have doubts about the DO's sincerity and his own understanding of the rules of the game. We will return to this in \secref{condition2}.

\subsection{Some meanings of ``probability'' \seclabel{tricky}}
Many thousands of pages have been written about the meaning of probability. We will not attempt a summary of the philosophical debate and instead highlight a few relevant points.

\subsubsection*{Frequency-in-an-ensemble interpretation of probability}
Consider the simple probabilistic statement: ``the probability of rain here tomorrow is 70\%.'' Tomorrow only happens once, so one might ask: in 70\% of what will it rain? The technical answer to this question is often: rain happens in 70\% of the members of an ensemble of computer simulations, run by a weather service, of what may happen tomorrow. So one interpretation of ``probability'' is ``relative frequency in a hypothetical ensemble of simulated possible futures.''

It is thus a statement about a model. How exactly it is linked to physical reality is not completely clear.

\subsubsection*{Frequency-over-time interpretation of probability}
In some situations, the statement ``70\% probability of rain here tomorrow'' refers to the relative frequency over time. Before the advent of computer models in weather forecasting, people used to compare today's measurements (of, say, wind and pressure) to those from the past -- weeks, months, or even years earlier. Forecasts were made on the assumption that the weather tomorrow would resemble the weather that had followed similar conditions in the historical record.

Rather than a statement about outcomes of an \textit{in silico} model, the statement may thus be a summary of real-world observations over a long time.

\subsubsection*{Degree-of-belief interpretation of probability}
No matter how ``probability'' relates to a frequentist physical statement, whether with respect to an ensemble of simultaneously possible futures or to a sequence of actual past futures, it also corresponds to a mental state of believing something with a degree of conviction: ``I'm 90\% sure I left my wallet in that taxi.''

For our purpose it suffices to say that there's no guarantee that a probabilistic statement will be interpreted by the receiver (the DM) as it was intended by whoever made the statement (the DO).

\subsection{Consistent differences between DO and DM \seclabel{condition2}}

\subsubsection*{Estimation errors for probabilities}
Let's assume that both the DO and the DM mean by ``probability'' the relative frequency of an event in an infinitely long time series of observations. Of course, real time series have finite length, so probabilities defined this way are model parameters and cannot actually be observed. But, from a real time series, we can estimate the best values to put into a model, by counting how often we see an event.

As the probability of an event gets smaller, so does the number of times we see it in a finite time series. If we want to say something about the uncertainty in this number, we can measure it -- or imagine measuring it -- in several time series to see how much it varies. The variations from one time series to another get smaller for rarer events, but the \textit{relative} variations get larger, and so does the relative uncertainty in our estimate of probabilities. Take an extreme simplified example: asymptotically an event occurs in 0.1\% of observations, and we have a time series of 100 observations.
Around 99.5\% of such time series will contain 0 or 1 events. Na\"{i}vely, then, we would estimate the probability as either 0 or 1\%. In other words, we would estimate the event as either impossible or occurring ten times more frequently than it really would in a long series. However, if the event occurs 50\% of the time asymptotically, then around 99.5\% of time series would contain between 35 and 65 events, leading to a much smaller relative error in probability estimates.

A DM who must estimate probabilities from observations is well advised to account for this behaviour of uncertainties in his decision making. Specifically, the DM should acknowledge that, due to his lack of information, \textit{prima facie} rare events may be rather more common than his data suggest, while common events, being revealed more often, are more easily characterised. In such circumstances, caution may dictate that the DM assign to rare events higher probabilities than his estimates, commensurate with his uncertainty in them. This would look like probability weighting to a DO and, indeed, would constitute a mechanistic reason for it.\footnote{Interestingly, \textcite[281]{KahnemanTversky1979} made the same point, noting that ``overestimation that is commonly found in the assessment of the probability of rare events'' has the same effect on human decisions as probability weighting. Since they assumed that subjects in experiments adopt unquestioningly the stated probabilities, they argued that probability weighting was necessary to explain their observations. We make no such assumption here.}

Formalising these thoughts, we find that so long as relative uncertainties are larger for rare events than for common events -- which, generically, they are -- then an inverse-S curve emerges. See \Appref{relative_errors} for a detailed discussion. Here we make a simple scaling argument and then check it with a simulation. For an asymptotic probability density $p(x)$, the number of events $n(x)$ we see in the small interval $[x, x+ \delta x]$ in a time series of $T$ observations is proportional to $p(x)$, to $\delta x$, and to $T$. So we have $n(x) \sim p(x) \delta x T$, where we mean by $\sim$ ``scales like.'' We also know that such counts, for example in the simple Poissonian case, are random variables whose uncertainties scale like $\sqrt{n(x)}$.

If we knew the asymptotic probability density $p(x)$, we could make an estimate of the count as
\be
n(x) \approx p(x) \delta x T \pm \sqrt{p(x) \delta x T} ~.
\elabel{count_est}
\ee
We would write $\nhat(x) \equiv p(x) \delta x T$ as the estimate of $n(x)$ and $\err{\nhat(x)} \equiv \sqrt{p(x) \delta x T}$ as its uncertainty. Of course, this situation seldom applies, because usually we do not know $p(x)$.

Conversely, and more realistically, if we observe a count $n(x)$, then we can use the scaling $ p(x) \sim n(x)/T\delta x$ to make an estimate of the asymptotic probability density as
\be
p(x) \approx \frac{n(x)}{T\delta x} \pm \frac{\sqrt{n(x)}}{T \delta x} ~.
\elabel{prob_est}
\ee
We write $\phat(x) \equiv n(x)/T\delta x$ as the estimate of $p(x)$, and
\be
\err{\phat(x)} \equiv \frac{\sqrt{n(x)}}{T \delta x} = \sqrt{\frac{\phat(x)}{T \delta x}}
\ee
as its uncertainty, which we have expressed in terms of the estimate itself.

The standard error, $ \sqrt{\phat(x)/T \delta x}$, in an estimated probability density shrinks as the probability decreases. However, the relative error in the estimate is $1/\sqrt{\phat(x)T\delta x}$, which grows as the event becomes rarer. This is consistent with our claim, that low probabilities come with larger relative errors, and constitutes the key message of this section. Errors in probability estimates behave differently for low probabilities than for high probabilities: absolute errors are smaller for lower probabilities, but relative errors are larger.

Let's assume that the DM is aware of the uncertainties in his estimates and, furthermore, that he does not like surprises. To avoid surprises, he adds the standard error to his estimate of the probability density, $\phat(x)$, in order to construct his decision weight density, $w(x)$. In effect, he constructs a reasonable worst case for each of his estimates. After normalising, this conservative strategy yields generically,
\be
w(x) = \frac{\phat(x)+\err{\phat(x)}}{\int_{-\infty}^{\infty}\left(\phat(s)+\err{\phat(s)}\right)ds}~,
\elabel{weight_density_gen}
\ee
and specifically, for the type of uncertainty we consider,
\be
w(x)= \frac{\phat(x)+\sqrt{\frac{\phat(x)}{T \delta x}}}{\int_{-\infty}^{\infty}\left(\phat(s)+\sqrt{\frac{\phat(s)}{T \delta x}}\right)ds}~.
\elabel{weight_density_spec}
\ee

Note that the cautionary correction term in \eref{weight_density_spec} is parametrised by $T\delta x$, which scales like the number of observations in $[x, x+\delta x]$. As $T\delta x$ grows large, the correction vanishes and both $w(x)$ and $\phat(x)$ become consistent with the asymptotic density, $p(x)$. With perfect information, a DM need not adjust decisions to account for uncertainty.

Does our analysis, culminating in \eref{weight_density_gen} and \eref{weight_density_spec}, reproduce the stylised facts of probability weighting, in particular the inverse-S curve? We check in two ways. First, analytically, by applying the DM's cautionary correction in \eref{weight_density_spec} directly to reference probability density functions. Second, by simulating the DM compiling counts of outcomes drawn from reference distributions, from which he estimates probability densities and their uncertainties. The simulation is meant to explore how noisy the effect is when a DM really only sees a single time series. The Python code is available at \href{https://bit.ly/lml-pw-code-dm-count}{\texttt{bit.ly/lml-pw-code-dm-count}}, and a Jupyter notebook can be loaded to manipulate the code in an online environment at \href{https://bit.ly/lml-pw-dm-count-b}{\texttt{bit.ly/lml-pw-dm-count-b}}. In both cases, we treat the DO as using the reference distribution to make his predictions of the DM's behaviour.

\Fref{square_root_error} shows the resulting PDFs and CDF mappings generated by setting $\phat(x)$ in \eref{weight_density_spec} to be the probability density functions for a Gaussian distribution and a fat-tailed $t$-distribution. Inverse-S curves are found for both distributions and the effect is more pronounced for the fat-tailed distribution.
\begin{figure}[!htb]
\centering
\includegraphics[width=1.0\textwidth]{./figs/square_root_error.pdf}
\caption{\textbf{Mapping PDFs and CDFs with estimation errors.} PDFs (left) and inverse-S curves (right) arising when the DO assumes a Gaussian (scale 1, location 0, top line) or a $t$-distribution (shape 2, location 0, bottom line), and the DM uses decision weights according to \eref{weight_density_spec} with $T\delta x=10$. For the fat-tailed $t$-distribution (in the bottom line) the difference between $p(x)$ and $w(x)$ is more pronounced.}
\flabel{square_root_error}
\end{figure}

\Fref{dm_count_sim} shows the results of a computer simulation of a DM who observes a series of realisations of either Gaussian or $t$-distributed random variables, which he counts into bins. In the simulation, a probability density, $\phat(x)$, is estimated for each bin as $n(x)/T\delta x$ and its uncertainty, $\err{\phat(x)}$, is obtained numerically as standard deviation in each $\phat(x)$ over 1000 parallel simulations. The DM's decision weights are then obtained according to \eref{weight_density_gen}. Again, inverse-S curves are found for both distributions, corroborating our scaling arguments.
\begin{figure}[!htb]
\centering
\includegraphics[width=1.0\textwidth]{./figs/dm_count_sim.pdf}
\caption{\textbf{Simulations of a DM estimating probability densities by counting events in a finite series of observations.} Left: estimated probability densities for $T=100$ Gaussian (top; location 0, scale 2) and $t$-distributed (bottom; location 0, scale 1, shape 1.5) variates counted in bins of width $\delta x=0.4$. Red bars show the estimates, $\phat(x)$, and blue bars show the estimates with one standard error added, $\phat(x)+\err{\phat(x)}$. Right: inverse-S curves for a DO who assumes $p(x)$ follows a Gaussian (top) and $t$-distribution (bottom), while the DM uses decision weight density, $w(x)$, derived by normalising his conservative estimates (blue bars on left) according to \eref{weight_density_gen}.
}
\flabel{dm_count_sim}
\end{figure}

\subsubsection{Typical situations of DO and DM: ergodicity}
To recap: behavioural economists observe that DOs tend to assign lower weights to low-probability events than DMs. While behavioural economists commonly assume that the DM is wrong, we make no such judgement. In any decision problem, the aim of the decision must be taken into account. Crucially, this aim depends on the situation of the individual.

The two types of modellers (DO and DM) pursue different goals. In our thought experiment, the DO is a behavioural scientist without personal exposure to the success or failure of the DM, whom we imagine as a test subject or someone whose behaviour is being observed in the wild. The DM, of course, has such exposure. Throughout the history of economics, it has been a common mistake, by DOs, to assume that DMs optimise what happens to them on average in an ensemble. To the DM, what happens to the ensemble is seldom a primary concern. Instead, he is concerned with what happens to him over time. Not distinguishing between these two perspectives is only permissible if they lead to identical predictions, meaning only if the relevant observables are ergodic \parencite{Peters2019b}.

It is now well known that this is usually not the case in the following sense: DMs are usually observed making choices that affect their wealth, and wealth is usually modelled as a stochastic process that is not ergodic. The ensemble average of wealth does not behave like the time average of wealth.

The most striking example is the universally important case of noisy multiplicative growth, the simplest model of which is geometric Brownian motion, $dx=x(\mu dt+\sigma dW)$. In the present context of human economic decisions, this is the most widely used model of the evolution of invested wealth. The average over the full statistical ensemble (often studied by the DO) of geometric Brownian motion grows as $\exp(\mu t)$. Each individual trajectory, on the other hand, grows in the long run as $\exp[(\mu-\frac{\sigma^2}{2})t]$. If the DO takes the ensemble perspective, he will deem the fluctuations irrelevant whereas, from the DM's time perspective, they reduce growth. So, while a DO curious about the ensemble may suffer no consequences from disregarding rare events, hedging against such events is central to the DM's success.

The difference between how these two perspectives evaluate the effects of probabilistic events is qualitatively in line with the observed phenomena we set out to explain. The DM typically has large uncertainties, especially for low-probability events, and has an evolutionary incentive to err on the side of caution, \ie to behave as though extreme events have a higher probability than in the DO's model.

\section{Fitting the model to experimental results \seclabel{Fitting_the}}
Visually, looking at the figures and the level of noise in the data in \fref{TK1992}, one would conclude that Tversky and Kahneman's physically unmotivated function, $\tilde{F}^{TK}_w(F_p)$ in \eref{correspondence}, fits the data no more efficiently than the functions arising from our mechanistic model. This is particularly evident in the bottom panels of \fref{Gauss_scale_location_both_KT}, which show that a Gaussian, $w(x)$, whose scale and location differ from those of $p(x)$, reproduces the fitted functional shape of $\tilde{F}^{TK}_w(F_p)$.

For completeness and scientific hygiene, in the present section we fit location and scale parameters in the Gaussian and $t$ models for $F_w$ to experimental data from \textcite{TverskyKahneman1992} (depicted in circles in \fref{TK1992}) and from \textcite{TverskyFox1995}. Specifically, in the Gaussian model we fit the location and scale parameters $\mu$ and $\sigma$ in the CDF,
%
\be
F_w\left(x\right) = \Phi\left(\frac{\Phi^{-1}\left(F_p\left(x\right)\right) - \mu}{\sigma}\right)~,
\ee
%
where $\Phi$ is the CDF of the standard normal distribution. In the $t$-model, we fit the location and shape parameters, $\mu$ and $\nu$, in the CDF, $F_w(x)$, of a $t$-distributed random variable (see \secref{Different_shapes}). In both cases, we assume that $F_p(x)$ is that of a standard normal distribution.

In addition to \eref{correspondence} used by Tversky and Kahneman, we fit the function
%
\be
\tilde{F}^{L}_w\left(F_p; \delta,\gamma\right) =\frac{\delta F_p^{\gamma}}{\delta F_p^{\gamma} + \left(1-F_p\right)^{\gamma}}\,,
\elabel{LattimoreFunction}
\ee
%
suggested by \textcite{LattimoreBakerWitte1992} to parametrically describe probability weighting (also used by \textcite{TverskyWakker1995} and \textcite{Prelec1998}). The reason for fitting \eref{LattimoreFunction} is to ensure a fair comparison: the Gaussian and $t$ models are characterised by two parameters, whereas \eref{correspondence} only has one free parameter. \Eref{LattimoreFunction} has two parameters.

\Fref{curvefit} presents the fit results. We obtain very good fits to data for both Gaussian and $t$-distributions, as well as for \eref{correspondence} and \eref{LattimoreFunction}, in the two experiments. It is practically impossible to distinguish between the fitted functions within standard errors. We conclude that our model fits the data well, and unlike \eref{correspondence} or \eref{LattimoreFunction}, the fitted functions are directly derived from a physically plausible mechanism. They are not simply phenomenological.

\begin{figure}[!htb]
\centering
\includegraphics[width=1.0\textwidth]{./figs/curvefit.pdf}
\caption{\textbf{Model fitting to experimental data from \textcite{TverskyKahneman1992} (left) and \textcite{TverskyFox1995} (right).}
Left: \textcite{LattimoreBakerWitte1992} \eref{LattimoreFunction}: $\delta=0.67\,\left(SE = 0.04\right)$, $\gamma=0.58\,\left(\pm0.03\right)$; Gaussian model: $\mu=0.38\,\left(\pm0.06\right)$, $\sigma=1.60\,\left(\pm0.10\right)$; $t$-model: $\nu=1.27\,\left(\pm0.28\right)$, $\mu=0.40\,\left(\pm0.07\right)$; \textcite{TverskyKahneman1992} \eref{correspondence}: $\gamma=0.60\,\left(\pm0.02\right)$. Right: \textcite{LattimoreBakerWitte1992}: $\delta=0.77\,\left(\pm0.01\right)$, $\gamma=0.69\,\left(\pm0.01\right)$; Gaussian model: $\mu=0.22\,\left(\pm0.01\right)$, $\sigma=1.41\,\left(\pm0.03\right)$; $t$-model: $\nu=1.41\,\left(\pm0.21\right)$, $\mu=0.22\,\left(\pm0.03\right)$; \textcite{TverskyKahneman1992}: $\gamma=0.68\,\left(\pm0.01\right)$. Shaded areas indicate two standard errors in the fitted parameter values. The fit was done by implementing the Levenberg-Marquardt algorithm \parencite{Levenberg1944} for non-linear least squares curve fitting.
}
\flabel{curvefit}
\end{figure}

\section{Discussion}

On 28 February 2020, \textcite{Sunstein2020}, a behavioural economist, legal scholar, and former United States Administrator of the Office of Information and Regulatory Affairs, diagnosed that people's concern about a potential coronavirus outbreak in the US was attributable to an extreme case of probability weighting. Supposedly, according to Sunstein, people were neglecting the fact that such an event had a low probability. When the piece was published, many commented that it seemed quite reasonable to them to take precautions, and that Sunstein himself may have underestimated both the severity and likelihood of what lay ahead. One month later, the US suffered a major outbreak of coronavirus.

This sad episode illustrates that an inverted S-curve is a neutral indicator of a difference in opinion. It says nothing about who is right and who is wrong.

The term ``probability weighting'' suggests an obscure mental process, where a DM carries out operations on probabilities. It seems more natural to us to consider a DM modelling events about whose probabilities he is unsure. From this latter point of view, it is easy to think of reasons for a DM's model to differ from a DO's. DMs will often have cause to include additional uncertainty, leading to the frequently observed inverse-S curve.

The model of estimating probabilities from real time series, which we discuss in \secref{Reasons_for}, has qualitative features that display a degree of universality. Relative errors in the DM's probability estimates are always greater for rarer events. A dislike of the unexpected, which explains the systematic overestimation of low probabilities, is similarly common. ``Probability weighting'' is purely descriptive and comes with the ill-conceived connotation of DMs suffering from a cognitive error. The phenomenon is better thought of as DMs making wise decisions given the information available to them. Such information is necessarily limited because, for example, DMs are constrained to collect such information in time.

\newpage
\printbibliography

\appendix

\section{Inverse-S from relative errors in probabilities}
\seclabel{relative_errors}

For an inverse-S curve to emerge, small probability densities have to be overestimated ($w>p$) and large ones underestimated ($w<p$), as is indeed the case, for example in \fref{square_root_error}. Let's connect this statement to one about relative uncertainties. The decision weight is arrived at by adding the probability $p(x)$ to its uncertainty $\err{p(x)}$ and normalising, as we did in \eref{weight_density_gen}, \ie
\be
w(x)=\frac{p(x)+\err{p(x)}}{\int_{-\infty}^{\infty} \left( p(s)+\err{p(s)} \right) ds}~.
\ee
This can be expressed as
\be
w(x)=p(x) \left(\frac{1+\frac{\err{p(x)}}{p(x)}}{\int_{-\infty}^{\infty} p(s)\left\{1+\frac{\err{p(s)}}{p(s)}\right\} ds}\right)~,
\elabel{rel_error}
\ee
where $\frac{\err{p(x)}}{p(x)}$ is the relative error, and the denominator of  \eref{rel_error} is a normalisation constant. If the relative error is larger for small probabilities than for large probabilities, then small probabilities are enhanced more (the summand $\frac{\err{p(x)}}{p(x)}$ in the numerator is greater) than large probabilities. The normalisation constant scales down all probabilities equally, and where the enhancement was greater, $w(x)$ ends up above $p(x)$, and where it was lower $w(x)$ ends up below $p(x)$. So, if the relative error is larger for small probabilities, an inverse-S curve emerges.

We can say one more thing about this procedure. If an inverse-S curve exists, then $p(x)$ and $w(x)$ cross somewhere, see \fref{square_root_error}. This happens when the relative error attains its expectation value (with respect to the density $p$). Rewriting \eref{rel_error} as
\be
w(x)=p(x) \left(\frac{1+\frac{\err{p}}{p}}{1+\ave{\frac{\err{p}}{p}}}\right),
\ee
we see that $w(x)=p(x)$ when $\frac{\err{p}}{p}=\ave{\frac{\err{p}}{p}}.$

\section{The emergence of an S-shaped curve}
\seclabel{Scurve}

We saw how the robust qualitative observation of the inverse-S shape in \fref{TK1992} emerges when the DM uses a larger scale in his model of the world than the DO. In the Gaussian it was possible to express the DM's decision weight $w$ as a function of probability $p$:
\be
w(p)= p^{\frac{1}{\alpha^2}} \frac{\left(2\pi\sigma^2\right)^{\frac{1-\alpha^2}{2\alpha^2}}}{\alpha} ~.
\ee

If the DM uses a larger scale in his model than the DO, then $\alpha>1$, and an inverse-S shaped curve emerges when plotting the CDFs $F_w$ and $F_p$. Yet, it is also possible that the DM uses a smaller scale than the DO, \ie $\alpha<1$. This can occur, for example, if the DM is an insider and has more information on the particular decision problem than provided to him by the DO. Instead of an inverse-S shape this would lead to an S-shaped curve, as shown in \fref{mapping_pdfs_cdfs_Sshape} (compare with \fref{mapping_pdfs} and \fref{mapping_cdfs} for $\alpha>1$).

\begin{figure}[!htb]
\centering
\includegraphics[width=\textwidth]{./figs/mapping_pdfs_cdfs_Sshape.pdf}
\caption{\textbf{Mapping PDFs and CDFs when $\alpha<1$.} Top left: probability PDF (red), estimated by a DO; and decision-weight PDF (blue), estimated by a DM. The DO models $x$ with a best estimate for the scale (standard deviation) and assumes the true frequency distribution is the red line. The DM models $x$ with a smaller scale ($\alpha=1/2$), and assumes the true frequency distribution is the blue line. Comparing the two curves, the DM appears to the DO as someone who over-estimates probabilities of low-probability events and underestimates probabilities of high-probability events, indicated by vertical arrows.
Top right: the difference between decision weights and probabilities can be expressed by plotting the decision weight \vs the probability observed at $x$. This corresponds to a non-linear distortion of the horizontal axis. The arrows on the left correspond to the same $x$-values as on the right, and thus start and end at identical vertical positions as on the left. They are shifted to different locations horizontally, due to the non-linear distortion of the horizontal axis.
Bottom left: The DO assumes the observable $X$ follows Gaussian distribution $X \sim \ND(0,1)$, which results in the red CDF of the standard normal, $F_p(x) = \Phi_{0,1}(x)$. The DM is less cautious or have more information, so in his model $X$ follows a narrower Gaussian distribution, $X \sim \ND(0,1/4)$ depicted by $F_w(x)$ (blue).
Following the vertical arrows (left to right), we see that for low values of the event probability $x$ the DM's CDF is smaller than the DO's CDF, $F_p(x) > F_w(x)$; the curves coincide at 0.5 because no difference in location is assumed; necessarily for large values of the event probability $x$ the DM's CDF must be higher than the DO's.
Bottom right: the same CDFs as on the left but now plotted not against $x$ but against the CDF $F_p$. Trivially, the CDF $F_p$ plotted against itself is the diagonal; the CDF $F_w$ now displays an S-shaped curve rather than the typical inverse-S curve. The arrows start and end at the same vertical values as on the left. Compare figure with \fref{mapping_pdfs} and \fref{mapping_cdfs} for $\alpha>1$.}
\flabel{mapping_pdfs_cdfs_Sshape}
\end{figure}

\end{document}