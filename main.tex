% Convention
% * British English
% * Footnotes are sentences, not only references in a footnote

\documentclass[a4paper, 12pt]{article}

\usepackage{amsthm, amsmath, amssymb, mathrsfs, multirow, url}
\usepackage{amsfonts} 
\usepackage[blocks,auth-sc-lg,affil-it]{authblk}
\renewcommand\Affilfont{\itshape\small}
\usepackage{caption}
\usepackage{csquotes}
\usepackage{subcaption}
\usepackage{marvosym}
\usepackage{makecell,booktabs,siunitx}
\usepackage{graphicx} 
\usepackage{ifthen} 
\usepackage[usenames]{color}
\usepackage{placeins}

\usepackage[margin=1in]{geometry}
\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{\scalebox{0.25}{
\includegraphics{resone-logo.png}}
\rhead{
{\small 
\url{https://www.researchers.one/article/2020-04-14}}
% \url{https://bit.ly/LML-pw}} % fill in URL here
}
}

\usepackage[colorlinks=true,urlcolor=blue,linkcolor=red,citecolor=blue]{hyperref}

% =====================================================================
% =========================  BibLateX w/ BibTeX   =====================
% =====================================================================
\usepackage[
arxiv=abs,																					% Link to arXiv abstract page
autocite=inline,																		% structur of \autocite{key}
autolang=hyphen,																		% active if langid is set
% autopunct=false,																	%
backend=bibtex,																		% Backend choice BibTeX8 case-sensitive sorting 
backref=true,																				% page cited in the paper
% bibencoding=latin1,
bibstyle=authoryear,																% style in bibliography
% bibstyle=draft,																		% style in bibliography
% bibwarn=true,
citestyle=authoryear-comp,													% style in the document
dashed=false,																				% always repeat author names in bibliography
% -- hyperref=true,																		%
% language=autobib,																		% bibliography language same as language of origin
% sortcites=true,																			% sort citations in the text
sortcites=false,																		% do not sort citations in the text
% sorting=nyt,																				% sorting in bibliography
maxbibnames=11,																			% limit authors in bibliography to 11
maxcitenames=2,																			% start to use et alii
% minbibnames=10																			% Angabe wie vieler Autoren nach Schnitt
mincitenames=1,																			% cut to X authors with et al.
url=true,doi=true,eprint=true,											% print url,doi,eprint
useprefix=true,																			% consider nobiliary particle von, van, de, etc.
]{biblatex}

\makeatletter
\def\blx@maxline{77}
\makeatother

\addbibresource{../LML_bibliography/bibliography}

% =====================================================================
% ===================   STYLE OF THE REFERENCES   =====================
% =====================================================================

% \newcommand*\person[1]{\textsc{#1}}									% prints author names as small caps
\renewcommand{\mkbibnamegiven}[1]{\person{#1}}
\renewcommand{\mkbibnamefamily}[1]{\person{#1}}
\renewcommand{\mkbibnameprefix}[1]{\person{#1}}
% \renewcommand{\mkbibnamesuffix}[1]{\textsc{#1}}

\setlength{\bibitemsep}{1.5\itemsep}			% space between 2 bibitems

\DeclareFieldFormat{pages}{#1}		% removes pagination (p./pp.) before page numbers

% \renewbibmacro{etal}{\textsc{et al.}}  % find the name for this macro

\renewbibmacro{in:}{}					% print no 'In:' before the <Journal Name>'

% \DefineBibliographyStrings{english}{%
%     backrefpage  = {cit. on p.}, 			% default: for single page number
%     backrefpage  = {}, 								% for single page number
%     backrefpages = {cit. on pp.} 			% default: for multiple page numbers
%     backrefpages = {} 								% for multiple page numbers
% }
% =====================================================================

\usepackage{xspace}

\newcommand{\bi}{\begin{itemize}}
\newcommand{\ei}{\end{itemize}}

\newcommand{\ave}[1]{\left\langle#1 \right\rangle}
\newcommand{\gamble}[1]{\textsc{#1}}
\newcommand{\person}[1]{\textsc{#1}\xspace}
\newcommand{\concept}[1]{{\sc#1}}
\newcommand{\elabel}[1]{\label{eq:#1}}
\newcommand{\eref}[1]{(Eq.~\ref{eq:#1})}
\newcommand{\ceref}[2]{(\ref{eq:#1}#2)}
\newcommand{\Eref}[1]{Equation~(\ref{eq:#1})}
\newcommand{\flabel}[1]{\label{fig:#1}}
\newcommand{\fref}[1]{Fig.~\ref{fig:#1}}
\newcommand{\Fref}[1]{Figure~\ref{fig:#1}}
\newcommand{\tlabel}[1]{\label{tab:#1}}
\newcommand{\tref}[1]{Tab.~\ref{tab:#1}}
\newcommand{\Tref}[1]{Table~\ref{tab:#1}}
\newcommand{\seclabel}[1]{\label{sec:#1}}
\newcommand{\secref}[1]{Sec.~\ref{sec:#1}}
\newcommand{\Secref}[1]{Section~\ref{sec:#1}}
\newcommand{\appref}[1]{App.~\ref{sec:#1}}
\newcommand{\Appref}[1]{Appendix~\ref{sec:#1}}
\newcommand{\nmax}{\ensuremath{n_{\text{max}}}\xspace}
\newcommand{\gens}{\ensuremath{g_\text{e}}}
\newcommand{\gtime}{\bar{g}}
\newcommand{\gi}{g^{(i)}}
\newcommand{\Ito}{It\^{o}\xspace}

\newcommand{\etal}{\textit{et al.}\xspace}
\newcommand{\apriori}{\textit{a priori}\xspace}
\newcommand{\ia}{\textit{i.a.}\xspace}
\newcommand{\ie}{\textit{i.e.}\xspace}
\newcommand{\etc}{\textit{etc.}\xspace}
\newcommand{\eg}{\textit{e.g.}\xspace}
\newcommand{\Eg}{\textit{E.g.}\xspace}
\newcommand{\cf}{\textit{cf.}\xspace}
\newcommand{\vs}{\textit{vs.}\xspace}
\newcommand{\vv}{\textit{v.v.}\xspace }
\newcommand{\gensst}{g_{\text{est}}}
\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}
\newcommand{\bea}{\begin{eqnarray}}
\newcommand{\eea}{\end{eqnarray}}
\newcommand{\D}{\Delta}
\newcommand{\yn}[1]{{y^{(#1)}}}
\newcommand{\xin}[1]{{\xi^{(#1)}}}
\newcommand{\Wn}[1]{{W^{(#1)}}}
\newcommand{\mun}[1]{{\mu^{(#1)}}}
\newcommand{\sigman}[1]{{\sigma^{(#1)}}}
\newcommand{\sigmac}{{\sigma_\rho}}
\newcommand{\err}[1]{\varepsilon\left[#1\right]}
\newcommand{\phat}{\hat{p}}
\newcommand{\nhat}{\hat{n}}

\newcommand{\ND}{\mathcal{N}} % Normal Distribution
\newcommand{\sigmat}{\tilde{\sigma}}
%\newcommand{\var}[1]{\text{var}(#1)}
\newcommand{\MK}[1]{\textcolor{red}{\textit{***MK: #1 MK***}}}
\newcommand{\OP}[1]{{\it ***OP: #1 OP***}}
\newcommand{\YB}[1]{{\it ***YB: #1 YB***}}
\renewcommand{\AA}[1]{{\it ***AA: #1 AA***}}

\title{What are we weighting for? \\
{\normalsize A mechanistic model for probability weighting\thanks{The latest version of the manuscript is available at \href{https://bit.ly/lml-pw}{bit.ly/lml-pw}. The code to generate the figures and simulations is available at \href{https://bit.ly/lml-pw-code}{bit.ly/lml-pw-code}.}}}

% Authors with Email addresses as footnotes
\author[1,2]{Ole Peters\thanks{\texttt{\href{mailto:o.peters@lml.org.uk}{~\Letter~o.peters@lml.org.uk}}}}
\author[1]{Alexander Adamou\thanks{\texttt{\href{mailto:a.adamou@lml.org.uk}{~\Letter~a.adamou@lml.org.uk}}}}
\author[1,3,4]{Mark Kirstein\thanks{\texttt{\href{mailto:m.kirstein@lml.org.uk}{~\Letter~m.kirstein@lml.org.uk}}}}
\author[1,5]{Yonatan Berman\thanks{\texttt{\href{mailto:y.berman@lml.org.uk}{~\Letter~y.berman@lml.org.uk}}}}
% \affil[1]{London Mathematical Laboratory, 8 Margravine Gardens, London W6 8RH, UK}
\affil[1]{London Mathematical Laboratory, London, UK} % short version

% \affil[2]{Santa Fe Institute, 1399 Hyde Park Road, Santa Fe, 87501 NM, USA}
\affil[2]{Santa Fe Institute, NM, USA} % short version
% \affil[3]{Max-Planck-Institute for Mathematics in the Sciences, Inselstraße 22, D-04103 Leipzig, Germany}
\affil[3]{Max-Planck-Institute for Mathematics in the Sciences, Leipzig, Germany} % short version
% \affil[4]{Institute of Mathematics, Augustusplatz 10, D-04109 Leipzig University}
\affil[4]{Institute of Mathematics, Leipzig University, Leipzig, Germany} % short version
% \affil[5]{The Graduate Center and Stone Center on Socio-Economic Inequality, City University of New York}
\affil[5]{The Graduate Center and Stone Center on Socio-Economic Inequality, City University of New York, NY, USA}

\date{\today}

\begin{document}
\begin{titlepage}
	\maketitle
\thispagestyle{fancy}

\begin{abstract}
\noindent 
Behavioural economics provides labels for patterns in human economic behaviour. Probability weighting is one such label. It expresses a mismatch between probabilities used in a formal model of a decision (\ie model parameters) and probabilities inferred from real people's decisions (the same parameters estimated empirically). The inferred probabilities are called ``decision weights.'' It is considered a robust experimental finding that decision weights are higher than probabilities for rare events, and (necessarily, through normalisation) lower than probabilities for common events. Typically this is presented as a cognitive bias, \ie an error of judgement by the person.
Here we point out that the same observation can be described differently: broadly speaking, probability weighting means that a decision maker has greater uncertainty about the world than the observer.
We offer a plausible mechanism whereby such differences in uncertainty arise naturally: when a decision maker must estimate probabilities as frequencies in a time series while the observer knows them \textit{a priori}. This suggests an alternative presentation of probability weighting as a principled response by a decision maker to uncertainties unaccounted for in an observer's model.
\vspace{1em}

\noindent\textsf{\textbf{Keywords}} ~ Decision Theory, Prospect Theory, Probability Weighting, Ergodicity Economics
\vspace{.5em}

\noindent\textsf{\textbf{JEL Codes}} ~
\href{https://www.aeaweb.org/econlit/jelCodes.php?view=jel#C}{%
C61		% Optimization Techniques • Programming Models • Dynamic Analysis
$\cdot$
}%
\href{https://www.aeaweb.org/econlit/jelCodes.php?view=jel#D}{%
D01 	% Microeconomic Behaviour: Underlying Principles
$\cdot$
D81 	% Criteria for decision making under Risk and Uncertainty
}
\end{abstract}
\end{titlepage}
 
\setcounter{page}{2}		% titlepage=page 1, continue with page 2

\section{Introduction}
\textit{Probability weighting} is a concept that originated in prospect theory \parencite{KahnemanTversky1979,TverskyKahneman1992}. It is one way to conceptualise a pattern in human behaviour of caution with respect to formal models.
This is best explained by a thought experiment, in which  
\bi
	\item a \textit{disinterested observer} (DO), such as an experimenter, tells
	\item a \textit{decision maker} (DM)
\ei
that an event occurs with some probability. The DO observes the DM's behaviour (\eg gambling on the event) and finds it consistent with a behavioural model (\eg expected-utility optimization) in which the DM uses a probability that differs systematically from what the DO has declared. The apparent probabilities, inferred from the DM's decisions, are called ``\textit{decision weights}.'' We will adopt this nomenclature here.
% 
\bi
	\item By ``\textit{probabilities,}'' expressed as probability density functions (PDFs) and denoted $p(x)$, we will mean the numbers specified by a DO.
	\item By ``\textit{decision weights,}'' also expressed as PDFs and denoted $w(x)$, we will mean the numbers that best describe the behaviour of a DM in the DO's behavioural model.\footnote{In the literature, decision weights are not always normalised, but for simplicity we will work with normalised decision weights. Mathematically speaking, they are therefore proper probabilities even though we don't call them that. Our results are unaffected because normalising just means dividing by a constant (the sum or integral of the non-normalised decision weights).}
\ei
Here, $x$ is a realisation of a random variable, $X$. For example, $X$ might be the payout of a gamble which the DM is invited to accept or decline.

Different behavioural models may result in different inferred decision weights. Our focus is not on how these weights are inferred, but on the robust observation that decision weights, $w(x)$ (used by DMs), are higher than probabilities, $p(x)$ (declared by DOs), for extreme events, \ie when $p(x)$ is small. Thus, we do not consider any specific behavioural model: our aim is to find a general mechanistic explanation to probability weighting.

Probability weighting is often summarised visually by comparing 
\bi
\item cumulative density functions (CDFs) for probabilities, denoted 
\be
F_p(x)=\int_{-\infty}^x p(s) ds~,
\ee
\item and CDFs for decision weights, denoted
\be
F_w(x)=\int_{-\infty}^x w(s) ds~.
\ee
\ei
In \fref{TK1992} we reproduce the first such visual summary from \textcite[310]{TverskyKahneman1992}.
\begin{figure}[!htb]
\centering
\includegraphics[width=0.5\textwidth]{./figs/TK1992.pdf}
\caption{\textbf{Empirical phenomenon of probability weighting.} Cumulative decision weights $F_w$ (used by decision makers) versus cumulative probabilities $F_p$ (used by disinterested observers), as reported by \textcite[p.~310, Fig. 1, relabelled axes]{TverskyKahneman1992}. The figure is to be read as follows: pick a point along the horizontal axis (the cumulative probability $F_p$ used by a DO) and look up the corresponding value on the vertical axis of the dotted inverse-S curve (the cumulative decision weight $F_w$ used by a DM). Low cumulative probabilities (left) are exceeded by their corresponding cumulative decision weights, and for high cumulative probabilities it's the other way around. It's the inverse-S shape of the curve that indicates this qualitative relationship.}
\flabel{TK1992}
\end{figure}

Plotting $F_w$ as a function of $F_p$ generates a curve, whose generic shape we shall call the ``inverse-S''. The inverse-S is the main observational finding in probability weighting: it sits above the diagonal $F_w=F_p$ for events of low cumulative probability (such that $F_w>F_p$ for these events) and below the diagonal for events of high cumulative probability (such that $F_w<F_p$).

As a final piece of nomenclature, we will use the terms \textit{location}, \textit{scale}, and \textit{shape} when discussing probability distributions. Consider a standard normal distribution $\ND(0,1)$, whose parameters indicate location 0 and squared scale 1. (For a Gaussian, the location is the mean and scale is the standard deviation.) For a general random variable $X$, with arbitrary location parameter $\mu_X$ and scale parameter $\sigma_X$, the following transformation produces a standardised random variable with identically-shaped distribution, but with location 0 and scale 1:
\be
Z = \frac{X-\overbrace{\mu_X \mathstrut}^{\text{location}}}{\underbrace{\sigma_X\mathstrut}_{\text{scale}}}~.
\elabel{SN}
\ee
Thus the PDF of $Z$, $p(z)$, is a density function with location $\mu_Z=0$ and scale $\sigma_Z=1$. In a graph of a distribution, a change of location shifts the curve to the left or right, and a change in scale shrinks or blows up the width of its features. Neither operation changes the \textit{shape} of the distribution: two distributions have the same \textit{shape} if they can be made to coincide through a linear transformation of the form \eref{SN}.

\subsection{Related literature -- An emerging consensus?}
% What happened so far. The normative standard model
Normative models in decision theory prescribe what decision makers should do if they want to act in some sense optimally.
%
Since the 17th century rational choice has been codified as the maximisation of expectation values. 
%
However, lack of predictive power led to an abundance of paradoxes and puzzles in decision theory.
%
The response to disagreements between normative theory and empirical evidence has been ever since the ongoing transformation of the quantities contained in the optimand, \ie probabilities and monetary changes, but to leave the structure of the optimand as an expectation value untouched.

% Recipe: Add some degrees of freedom
Therefore, the dominant modelling mode in behavioural economics to address these paradoxes and puzzles is to add more degrees of freedom, to the expense of a loss of generality.
% Descriptive models
Such models are then labelled descriptive models of decision making. 
% How to add a DoF
Possible further degrees of freedom in descriptive models involve nonlinear transformations with additional free parameters.
% Justification of further DoF
Often these alterations of the normative model have no theoretical justification per se but are justified \textit{post hoc} by psychological considerations and their ability to fit the data.

Nowadays, expected utility theory (EUT) is the dominant normative model and codifies rational choice in terms of maximisation of the expectation value of the change in utility.
% No normative and descriptive model can ever exist
As EUT is irreconcilable with empirical evidence, parts of the community have grown accustomed to the received wisdom that ``no theory of choice can be both normatively adequate and descriptively accurate'' \autocite[p. S251]{TverskyKahneman1986}. This mindset has contributed to the dominance of \person{Kahneman \& Tversky}'s  ``biases and heuristics'' research programme in behavioural economics within the much broader spectrum of possibilities of boundedly rational decision making like for instance satisficing  explored in \textcite{Simon1956,Simon1959,SauermannSelten1962,Selten2002,GigerenzerSelten2002b}.
% DoF are biases
\textcite{Lopes1991} criticised that the ``heuristics \& biases'' programme  has led the discipline into a ``rhetoric of irrationality''. \textcite{Gigerenzer2018} raises a similar critique when he identifies a ``bias bias'' in behavioural economics. Judged from the perspective of the normative model all these transformations constitute deviations from the optimal behaviour and are thus negatively connoted as cognitive biases or errors of judgement which lead to poor decisions.


% Transition from general Descriptive Models to specific CPT
The core of the ``biases and heuristics'' research programme in behavioural economics and the
most influential descriptive model of decision making is cumulative prospect theory (CPT) developed in \textcite{KahnemanTversky1979,TverskyKahneman1992}.
% What is the content of PW
CPT was specifically designed to incorporate a stable empirical pattern, \ie the mismatch between (objective) probabilities in a normative model and the same empirically estimated parameters inferred from real people's decisions. The latter probabilities are also called ``decision weights''. If one chooses to plot the CDFs against each other as in \fref{TK1992} this mismatch takes its generic inverse-S shape. The consensus interpretation in the literature of this mismatch is that decision makers use higher decision weights than the objective probabilities in the normative model assign to rare events and (necessarily) assign lower decision weights for common events.
% Give it a name
This nonlinear transformation is labelled probability weighting and enters into CPT in terms of the probability weighting function. The inverse-S curvature of the probability weighting function is already in the original formulation of prospect theory in \textcite{KahnemanTversky1979} a key element to explain the fourfold pattern of risk attitudes, \ie risk aversion for gains and risk seeking for losses of high probability and risk seeking for gains and risk aversion for losses of low probability.
% Orthodox Literature
Since its inception, both CPT and probability weighting have attracted a lot of attention in the literature, prevailing are calibrations of its free parameters \autocite{LattimoreBakerWitte1992,TverskyFox1995,WuGonzalez1996,Prelec1998} and axiomatisations of CPT \autocite{WakkerTversky1993,TverskyWakker1995,ChateauneufWakker1999}. Only few studies question the origin of the additional degrees of freedom, among others \textcite{StewartETAL2015}. In a meta-analytic review, \textcite[Tab. 9]{WulffETAL2018} find evidence for all three possible forms of the probability weighting curve, alongside linear weighting (which amounts to no weighting) they survey studies that find regular as well as inverse-S curves.
% From PT to CPT
CPT was developed from mere prospect theory to overcome violations of stochastic dominance using rank-dependent probability weightings.
% Bernheim & Sprenger 2020 paragraph (if needed)
In a recent study \textcite{BernheimSprenger2020} confirm the inverse-S curve, but question the empirical validity of CPT because they find no evidence for the assumption of rank-dependent probability weighting.

% What this paper adds
Here we offer a statistical explanation of probability weighting, in the form of a mechanism which is capable of generating arbitrary mismatches between objective probabilities in a normative model and decision weights as a result of a generic inference problem solved by the DM. A DM needs to estimate model parameters from observations over time.
%
Depending on the direction of the mismatch arbitrary weighting curves are possible. The classical inverse-S curve emerges in the case where the DM cautiously incorporates more uncertainty into his model than a DO does in his model (see \fref{mapping_pdfs} \& \fref{mapping_cdfs}). Our statistical interpretation suggests that this behaviour is an adaptive strategy in decision making under uncertainty.
%
Our analysis in \fref{curvefit} suggests along with other studies \autocite{FoxHadar2006} that the statistical explanation of probability weighting is sufficient, at least if judged by its capability to fit the reported data. 
%
However, other studies showed that even after eliminating the sampling errors some form of nonlinear weighting persists, which suggests that some psychological effect exists beyond the statistical effect \autocite{FoxHadar2006,UngemachETAL2009}. \textcite{HauETAL2008} even go so far as to equate the overweighting in decisions-from-experience with a form of CPT plus a sampling error.
%
In contrast to other studies on the subject, we assume the DM to be aware of and to correct his estimate for his sampling error, whereas existing studies stop at the identification of the sampling error and thus believe the DM to use an underweighted und therefore biased estimate of rare events.

% Ecological/Adaptive Rationality
\paragraph{Ecological/Adaptive Rationality}
Our results on probability weighting as being beneficial are compatible with a body of work on ecological or adaptive rationality surveyed in \textcite{Smith2003a,GigerenzerSelten2002b,Selten2002,Todd2012}, that seeks to understand the ecological environment out of which observed behavioural strategies (such as probability weighting) evolved as adaptive behaviours.
% What is adaptive rationality
Often deviations from a normative model are found to be not maladaptive detrimental cognitive errors, but rather beneficial adaptive strategies that promote survival. The general question whose bias behavioural economics finds has been asked early on, see for example \textcite{Cohen1979a}. This line of research identified a whole ``adaptive toolbox'' \autocite{GigerenzerSelten2002} of such strategies sometimes called ``fast and frugal heuristics'' \autocite{GoldsteinGigerenzer2009,GigerenzerHertwigPachur2011}.
% Replication crisis and disappearing cognitive biases
Similar in spirit to our paper where we provide a mechanism for a behaviour which is commonly labelled as a ``cognitive bias'', many ``cognitive biases'' don't replicate \autocite{OSS2015}, the reverse effects are found as well \autocite[Tab. 1]{WulffETAL2018} or they disappear completely \autocite{Gigerenzer1991,GalesicETAL2012}.
\textcite{LiederETAL2018} explain probability weighting by recourse to the adaptive and effective use of limited cognitive resources. This suggests that the finding of many ``biases'' stems from imperfect measurement and from an inadequate understanding of human cognition and decision making.

% Decision from Description-Experience Gap
\paragraph{Description-Experience Gap}
Our results deduced from the two distinct perspectives of the DO and the DM also resonate with experimental results of the so-called description-experience gap in risky choice. This gap signifies the common finding of different choice behaviour depending on the sampling paradigm used in experiments. People either learn (\eg probabilities) by experiential sampling over time (decisions-from-experience) or learn from descriptions (decisions-from-description)\footcite{HertwigETAL2004,HertwigErev2009,ErevETAL2010,WulffETAL2018}. Our analysis of the perspective of the DM accords with the decisions-from-experience sampling paradigm.\footnote{Let us remark that in animal studies in behavioural ecology only the decisions-from-experience sampling paradigm is feasible.} However, the general finding of the description-experience gap is that in decisions-from-experience people underweighted rare events compared to decisions-from-description.\autocites[535]{HertwigETAL2004}[518]{HertwigErev2009} This is somewhat counterintuitive in relation to probability weighting\autocite[522]{HertwigErev2009}. \textcite{HauETAL2008,WulffETAL2018} report contradictory results and especially emphasise people's tendency to rely on small samples and that the associated large estimation errors of the inferred probabilities lead to stronger probability weighting, which therefore imply that estimation errors lead to regular-S shaped weighting curves. \textcite[Tab. 1]{WulffETAL2018} find all possible forms of weightings in the experience sampling paradigms.
%
In \secref{tricky} we agree that different representations together with different interpretations of probabilities carry inherent ambiguity about their cognitive processing. We go beyond this statement by analysing how the adaptive correction for estimation errors leads to consistent differences between the DO's and DM's model of the involved uncertainty.

%  Decision sampling
\paragraph{Decision sampling}
Within the decisions-from-experience paradigm, similar inference problems of the DM arise naturally and have been studied in the literature on decision sampling\footcite{Stewart2006,GalesicETAL2012,SeoETAL2019}. \Eg \textcite{Martins2006} explains probability weighting as a heuristic that approximates an evolutionary optimal Bayesian solution to the inference problem and is able to replicate inverse-S curves as well. Similar to our results, \textcite{SeoETAL2019} find that optimal beliefs deviate from the true Bayesian prior in a sequential learning problem. Thus it is optimal to overweight small priors and underweight large priors.

% Behavioural ecology studies ? 
% \footnote{\MK{Studies on optimal foraging in behavioural ecology show \ldots \footnote{Ask Kacelnik!} \ldots Kacelnik hasn't replied, yet}}

% EE paragraph
\paragraph{Ergodicity Economics}
Our work contributes to the growing field of ergodicity economics\autocite{Peters2019b} whose common approach is to explicitly embed decisions within experienced time of the DM. This embedding of decisions within time, leads to a change of the optimand such that the DM maximises the long-time growth rate of his resources, rather than expectation values of psychologically-transformed resource flows (possibly under psychologically-transformed probability measures).
% 
Therefore, this paper joins recent studies that demonstrate the explanatory power of the ergodicity economics and its ability to generate testable predictions, \eg on discounting\autocite{AdamouETAL2019}, risk preferences for time lotteries\autocite{BermanKirstein2020}, cooperation\autocite{PetersAdamou2015a}, leverage efficiency\autocite{PetersAdamou2020}.

By daring to change the optimand, ergodicity economics produces models and testable predictions, which ultimately decide whether the models are normative and descriptive at the same time. Experimental tests of the statistical interpretation need to control for the amount of uncertainty, but are feasible and may thus be used to design similar decisions-from-experience model comparisons like \textcite{MederETAL2019}, which controlled for wealth dynamics.

% Origin of Preferences
\paragraph{Evolutionary Origin of Risk Preferences}
We conclude this section with a line of research similar in spirit to ergodicity economics on the evolutionary origin of dynamically consistent risk preferences started by \textcite{Robson1996}. The point of departure of dominant models in economics theory is that successful strategies need to be selected for by evolution in terms of their fitness and first of all by their survival and not by preferences simply given in a model.\autocite{StiglerBecker1977} Those strategies which maximise expected profits are crowded out to the advantage of maximisers of the time-average growth rate. This work has recently been extended\footcite{SarverSadowski2019}, \eg \textcite{RobattoSzentes2017} investigate growth-optimal preferences on a population level. Although the authors still cling to the nonlinear transformation of utilities, they arrive at similar results in terms of agents that maximise the time-average growth rate of their resources. Therefore, we identify an emerging consensus in the literature spread over several disciplines to understand PW and similar empirical regularities as adaptive strategies once the ecological circumstances and cognitive limitations of the DM are taken properly into account. Furthermore, by embedding decision making within time, ergodicity economics provides a coherent framework for these findings.


\section{Probability weighting as a difference between models} \seclabel{ModelDiff}
Behavioural economics interprets \fref{TK1992} as evidence for a cognitive bias of the DM, an error of judgement. We will keep a neutral stance. We don't assume the DO to know ``the truth'' -- he has a model of the world. Nor do we assume the DM to know ``the truth'' -- he has another model of the world. From our perspective \fref{TK1992} merely shows that the two models differ. It says nothing about who is right or wrong.

\subsection{The inverse-S curve\seclabel{The_inverse}}
\subsubsection{Tversky and Kahneman}
\textcite{TverskyKahneman1992} chose to fit the empirical data in \fref{TK1992} with the following function,
%
\be
\elabel{correspondence}
\tilde{F}^{TK}_w\left(F_p; \gamma\right) = \left(F_p\right)^\gamma \frac{1}{\left[\left(F_p\right)^\gamma+\left(1-F_p\right)^\gamma\right]^{1/\gamma}} ~,
\ee
%
which maps from one CDF, $F_p$, to another, $F_w$. We note that no mechanistic motivation was given for fitting this specific family of CDF mappings, parameterised by $\gamma$. The motivation is purely phenomenological: with $\gamma<1$, this function can be made to fit to the data reasonably well.

The function $\tilde{F}^{TK}_w\left(F_p; \gamma \right)$ has one free parameter, $\gamma$. For $\gamma=1$ it is the identity, and the CDFs coincide, $\tilde{F}^{TK}_w\left(F_p\right)=F_p$. Further, $\tilde{F}^{TK}_w$ has the following property: any curvature moves the intersection with the diagonal away from the mid-point $1/2$. This means if the function is used to fit an inverse-S (where $\gamma<1$), the fitting procedure itself introduces a shift of the intersection to the left. We consider the key observation to be the inverse-S shape, whereas the shift to the left may be an artefact of the function chosen for the fit.

\subsubsection{Scale, location, and the inverse-S}
We now make explicit how the robust qualitative observation of the inverse-S shape in \fref{TK1992} emerges when the DM uses a larger scale in his model of the world than the DO.

We illustrate this with a Gaussian distribution.
Let's assume that a DO models an observable $x$ -- which will often be a future change in wealth -- as a Gaussian with location $\mu$ and variance $\sigma^2$. And let's further assume that a DM models the same observable as a Gaussian with the same location, $\mu$, but with a greater scale, so that the variance is $(\alpha\sigma)^2$. The DM simply assumes a broader range -- $\alpha$ times greater -- of plausible values, left panel of \fref{mapping_pdfs}.
\begin{figure}[!htb]
\centering
\includegraphics[width=\textwidth]{./figs/mapping_pdfs.pdf}
\caption{\textbf{Mapping PDFs.} Left: probability PDF (red), estimated by a DO; and decision-weight PDF (blue), estimated by a DM. The DO models $x$ with a best estimate for the scale (standard deviation) and assumes the true frequency distribution is the red line. The DM models $x$ with a greater scale (here 2 times greater, $\alpha=2$), and assumes the true frequency distribution is the blue line. Comparing the two curves, the DM appears to the DO as someone who over-estimates probabilities of low-probability events and underestimates probabilities of high-probability events, indicated by vertical arrows.
Right: the difference between decision weights and probabilities can also be expressed by directly plotting, for any value of $x$, the decision weight \vs the probability observed at $x$. This corresponds to a non-linear distortion of the horizontal axis. The arrows on the left correspond to the same $x$-values as on the right. They therefore start and end at identical vertical positions as on the left. Because of the non-linear distortion of the horizontal axis, they are shifted to different locations horizontally.}
\flabel{mapping_pdfs}
\end{figure}

If the DM uses a greater scale in his model, then decision weights are higher than probabilities for low-probability events, and (because of normalisation) lower than probabilities for high-probability events. We can express this by plotting, for any value of $x$, the decision weight \vs the probability observed at $x$, right panel of \fref{mapping_pdfs}.

In the Gaussian case we can write the distributions explicitly as
\be
w(x)=\frac{1}{\sqrt{2\pi (\alpha \sigma)^2}}\exp\left[\frac{-(x -\mu )^2}{2 (\alpha \sigma)^2}\right]~,
\elabel{DecisionW}
\ee
and
\be
p(x)=\frac{1}{\sqrt{2\pi \sigma^2}}\exp\left[\frac{-(x -\mu )^2}{2 \sigma^2}\right] ~.
\elabel{p}
\ee
Eliminating $(x-\mu)^2$ from \eref{DecisionW} and \eref{p} yields the following expression for decision weight as a function of probability:
\be
w(p)= p^{\frac{1}{\alpha^2}} \frac{\left(2\pi\sigma^2\right)^{\frac{1-\alpha^2}{2\alpha^2}}}{\alpha} ~.
\elabel{w_of_p}
\ee
We plot this in the right panel of \fref{mapping_pdfs}. As a sanity check, consider the shape of the $w(p)$ (blue curve, right panel \fref{mapping_pdfs}): for a given value of $\alpha$, it is just a power law in $p$ with some pre-factor that ensures normalization. If $\alpha>1$ it means that the DM uses a greater standard deviation than the DO. In this case, the exponent of $p$ satisfies $\frac{1}{\alpha^2}<1$, and the blue curve is above the diagonal for small densities and below it for large densities.

Alternatively, we can express the difference between models by plotting the CDFs $F_w$ and $F_p$. We do this in \fref{mapping_cdfs}, where the inverse-S emerges purely from the DM's greater assumed scale, $\alpha \sigma$.
\begin{figure}[!htb]
\centering
\includegraphics[width=\textwidth]{./figs/mapping_cdfs.pdf}
\caption{\textbf{Mapping CDFs.}
Left: The DO assumes the observable $X$ follows Gaussian distribution $X \sim \ND(0,1)$, which results in the red CDF of the standard normal, $F_p(x) = \Phi_{0,1}(x)$. The DM is more cautious, in his model the same observable $X$ follows a wider Gaussian distribution, $X \sim \ND(0,4)$ depicted by $F_w(x)$ (blue).
%
Following the vertical arrows (left to right), we see that for low values of the event probability $x$ the DM's CDF is larger than the DO's CDF, $F_p(x) < F_w(x)$; the curves coincide at 0.5 because no difference in location is assumed; necessarily for large values of the event probability $x$ the DM's CDF must be lower than the DO's.
Right: the same CDFs as on the left but now plotted not against $x$ but against the CDF $F_p$. Trivially, the CDF $F_p$ plotted against itself is the diagonal; the CDF $F_w$ now displays the generic inverse-S shape known from prospect theory. The arrows start and end at the same vertical values as on the left.
}
\flabel{mapping_cdfs}
\end{figure}

\FloatBarrier

\subsection{Different scales and locations\seclabel{A_mismatch}}

In \fref{Gauss_scale_location_both_KT} we explore what happens if both the scales and the locations of the DO's and DM's models differ. Visually, this produces an excellent fit to empirical data, to which we will return in \secref{Fitting_the}.
\begin{figure}[!htb]
\centering
\includegraphics[width=1.0\textwidth]{./figs/Gauss_scale_location_both_KT.pdf}
\caption{\textbf{CDF maps for Gaussian distributions.} Top left: Difference in location. DO assumes location 0, scale 1; DM assumes location 0.23 (bigger than DO), scale 1. Top right: Difference in scale. DO assumes location 0, scale 1; DM assumes location 0, scale 1.64 (broader than DO). Bottom left: Differences in scale and location. DO assumes location 0, scale 1; DM assumes location 0.23 (bigger than DO), scale 1.64 (broader than DO). Bottom right: Fit to observations reported by \textcite{TverskyKahneman1992}. This is \eref{correspondence} with $\gamma=0.65$. Note the similarity to bottom left.}
\flabel{Gauss_scale_location_both_KT}
\end{figure}
A difference in assumed scales and locations, for simple Gaussian distributions, is sufficient to reproduce the observations. This suggests a different nomenclature and a conceptual clarification. The inverse-S curve does not mean that ``probabilities are re-weighted.'' It means only that experimenters and their subjects have different views about appropriate models of, and responses to, a situation.

\FloatBarrier

\subsection{Different shapes\seclabel{Different_shapes}}
Numerically, our procedure can be applied to arbitrary distributions:
\begin{enumerate}
\item
construct a list of values for the CDF assumed by the DO, $F_p(x)$.
\item
construct a list of values for the CDF assumed by the DM, $F_w(x)$.
\item
plot $F_w(x)$ \vs $F_p(x)$.
\end{enumerate}
Of course, the DM could even assume a distribution whose shape differs from that of the DO's distribution.
The inverse-S arises whenever a DM assumes a greater scale for a unimodal distribution.
To illustrate the generality of the procedure, in \fref{Student-t} we carry it out for Student's (power-law tailed) $t$-distributions (which we refer to as $t$-distributions), where DO and DM use different shape parameters and different locations
\footnote{
The PDF of the $t$-distribution is
%
\be
p\left(x\right) = \frac{\Gamma\left(\frac{\nu+1}{2}\right)} {\Gamma\left(\frac{\nu}{2}\right)\sqrt{\pi\nu}\sigma} \left(1+\frac{1}{\nu}\left(\frac{x-\mu}{\sigma}\right)^2 \right)^{-\frac{\nu+1}{2}}\,,
\ee
%
where $\nu$ is the shape parameter, $\sigma$ is the scale parameter, and $\mu$ is the location parameter. The corresponding CDF is
%
\be
F\left(x\right) =
\begin{cases}
1 - \frac{1}{2} I_{\frac{\nu}{\left(\frac{x-\mu}{\sigma}\right)^2 + \nu}}\left(\frac{\nu}{2},\frac{1}{2}\right) &\text{ if } x-\mu \geq 0\,;\\
\frac{1}{2} I_{\frac{\nu}{\left(\frac{x-\mu}{\sigma}\right)^2 + \nu}}\left(\frac{\nu}{2},\frac{1}{2}\right) &\text{ if } x-\mu < 0\,,
\end{cases}
\ee
%
where $I_x\left(a,b\right)$ is the incomplete beta function.

In the limit $\nu \rightarrow \infty$, the $t$-distribution converges to a Gaussian with location $\mu$ and scale $\sigma$. We assume by default that $\sigma = 1$, so the $t$-distribution is effectively characterised by two parameters: shape ($\nu$) and location ($\mu$).
}
The result is qualitatively similar to the bottom right panel of \fref{mapping_cdfs}, corresponding to \eref{correspondence}.
\begin{figure}[!htb]
\centering
\includegraphics[width=0.5\textwidth]{./figs/Student-t.pdf}
\caption{\textbf{Probability weighting for $t$-distributions.} The DM uses different shape and location parameters (1 and 0.35 respectively) from those of the DO (3 and 0.2).}
\flabel{Student-t}
\end{figure}

\section{Reasons for different models\seclabel{Reasons_for}}
Probability weighting is usually interpreted as a cognitive bias that leads to errors of judgement and poor decisions by DMs.\footnote{Indeed, its originators presented it as such.
Introducing prospect theory, \textcite[277]{KahnemanTversky1979} wrote ``we are compelled to assume [\dots] that decision weights do not coincide with stated probabilities. These departures from expected utility theory must lead to normatively unacceptable consequences''.
They classified prospect theory as descriptive rather than normative, \ie as relating to actual rather than optimal behaviour \parencite[p. S252]{TverskyKahneman1986}.
Put simply, prospect theory aims to model systematic errors in human decision making, arising (in part) from inappropriate psychological adjustments of known probabilities.}
We caution against this interpretation.
At least we should keep in mind that it is unclear who suffers from the bias: experimenter or test subject (or neither or both)?
We are by no means the first to raise this question.
Commenting on another so-called cognitive bias regarding probabilities, the representativeness fallacy, \textcite{Cohen1979a} asked: ``Whose is the fallacy?''

Whatever the answer, two observations are robust and interesting: first, disagreement is common; and, second, the disagreement tends to go in the same direction, with DMs assuming a greater range of plausible outcomes than DOs.

An explanation for the first observation is that probability is a slippery concept and the word is used to mean different things. This suggests that phrasing information about probabilities concretely should reduce disagreement between DO and DM. For example, the statement ``10 out of 100 people have this disease'' conveys more, and more precise, information than ``the probability of having this disease is 0.1.'' Specifically, it tells us that a sample of people has been observed and what the size of the sample is.

Furthermore, \textcite{Gigerenzer2018} argues that statements involving integer counts, or what he calls natural frequencies,  (``10 out of 100'') are more readily understood by people than statements involving fractional probabilities (``0.1'').

The second observation may be explained as follows. A DO often has control over, and essentially perfect knowledge of, the decision problem he poses. A DM does not have such knowledge, and this ignorance will often translate into additional assumed uncertainty. For example, the DO may know the true probabilities of some gamble in an experiment, while the DM may have doubts about the DO's sincerity and his own understanding of the rules of the game. We will return to this in \secref{condition2}.

\subsection{Some meanings of ``probability'' \seclabel{tricky}}
Many thousands of pages have been written about the meaning of probability. We will not attempt a summary of the philosophical debate and instead highlight a few relevant points.

\subsubsection*{Frequency-in-an-ensemble interpretation of probability}
Consider the simple probabilistic statement: ``the probability of rain here tomorrow is 70\%.'' Tomorrow only happens once, so one might ask: in 70\% of what will it rain? The technical answer to this question is often: rain happens in 70\% of the members of an ensemble of computer simulations, run by a weather service, of what may happen tomorrow. So one interpretation of ``probability'' is ``relative frequency in a hypothetical ensemble of simulated possible futures.''

It is thus a statement about a model. How exactly it is linked to physical reality is not completely clear.

\subsubsection*{Frequency-over-time interpretation of probability}
In some situations, the statement ``70\% probability of rain here tomorrow'' refers to the relative frequency over time. Before the advent of computer models in weather forecasting, people used to compare today's measurements (of, say, wind and pressure) to those from the past -- weeks, months, or even years earlier. Forecasts were made on the assumption that the weather tomorrow would resemble the weather that had followed similar conditions in the historical record.

Rather than a statement about outcomes of an \textit{in silico} model, the statement may thus be a summary of real-world observations over a long time.

\subsubsection*{Degree-of-belief interpretation of probability}
No matter how ``probability'' relates to a frequentist physical statement, whether with respect to an ensemble of simultaneously possible futures or to a sequence of actual past futures, it also corresponds to a mental state of believing something with a degree of conviction: ``I'm 90\% sure I left my wallet in that taxi.''

For our purpose it suffices to say that there's no guarantee that a probabilistic statement will be interpreted by the receiver (the DM) as it was intended by whoever made the statement (the DO).

\subsection{Consistent differences between DO and DM \seclabel{condition2}}

\subsubsection*{Estimation errors for probabilities}
Let's assume that both the DO and the DM mean by ``probability'' the relative frequency of an event in an infinitely long time series of observations. Of course, real time series have finite length, so probabilities defined this way are model parameters and cannot actually be observed. But, from a real time series, we can estimate the best values to put into a model, by counting how often we see an event.

As the probability of an event gets smaller, so does the number of times we see it in a finite time series. If we want to say something about the uncertainty in this number, we can measure it -- or imagine measuring it -- in several time series to see how much it varies. The variations from one time series to another get smaller for rarer events, but the \textit{relative} variations get larger, and so does the relative uncertainty in our estimate of probabilities. Take an extreme simplified example: asymptotically an event occurs in 0.1\% of observations, and we have a time series of 100 observations.
Around 99.5\% of such time series will contain 0 or 1 events. Na\"{i}vely, then, we would estimate the probability as either 0 or 1\%. In other words, we would estimate the event as either impossible or occurring ten times more frequently than it really would in a long series. However, if the event occurs 50\% of the time asymptotically, then around 99.5\% of time series would contain between 35 and 65 events, leading to a much smaller relative error in probability estimates.

A DM who must estimate probabilities from observations is well advised to account for this behaviour of uncertainties in his decision making. Specifically, the DM should acknowledge that, due to his lack of information, \textit{prima facie} rare events may be rather more common than his data suggest, while common events, being revealed more often, are more easily characterised. In such circumstances, caution may dictate that the DM assign to rare events higher probabilities than his estimates, commensurate with his uncertainty in them. This would look like probability weighting to a DO and, indeed, would constitute a mechanistic reason for it.\footnote{Interestingly, \textcite[281]{KahnemanTversky1979} made the same point, noting that ``overestimation that is commonly found in the assessment of the probability of rare events'' has the same effect on human decisions as probability weighting. Since they assumed that subjects in experiments adopt unquestioningly the stated probabilities, they argued that probability weighting was necessary to explain their observations. We make no such assumption here.}

Formalising these thoughts, we find that so long as relative uncertainties are larger for rare events than for common events -- which, generically, they are -- then an inverse-S curve emerges. See \Appref{relative_errors} for a detailed discussion. Here we make a simple scaling argument and then check it with a simulation. For an asymptotic probability density $p(x)$, the number of events $n(x)$ we see in the small interval $[x, x+ \delta x]$ in a time series of $T$ observations is proportional to $p(x)$, to $\delta x$, and to $T$. So we have $n(x) \sim p(x) \delta x T$, where we mean by $\sim$ ``scales like.'' We also know that such counts, for example in the simple Poissonian case, are random variables whose uncertainties scale like $\sqrt{n(x)}$.

If we knew the asymptotic probability density $p(x)$, we could make an estimate of the count as
\be
n(x) \approx p(x) \delta x T \pm \sqrt{p(x) \delta x T} ~.
\elabel{count_est}
\ee
We would write $\nhat(x) \equiv p(x) \delta x T$ as the estimate of $n(x)$ and $\err{\nhat(x)} \equiv \sqrt{p(x) \delta x T}$ as its uncertainty. Of course, this situation seldom applies, because usually we do not know $p(x)$.

Conversely, and more realistically, if we observe a count $n(x)$, then we can use the scaling $ p(x) \sim n(x)/T\delta x$ to make an estimate of the asymptotic probability density as
\be
p(x) \approx \frac{n(x)}{T\delta x} \pm \frac{\sqrt{n(x)}}{T \delta x} ~.
\elabel{prob_est}
\ee
We write $\phat(x) \equiv n(x)/T\delta x$ as the estimate of $p(x)$, and
\be
\err{\phat(x)} \equiv \frac{\sqrt{n(x)}}{T \delta x} = \sqrt{\frac{\phat(x)}{T \delta x}}
\ee
as its uncertainty, which we have expressed in terms of the estimate itself.

The standard error, $ \sqrt{\phat(x)/T \delta x}$, in an estimated probability density shrinks as the probability decreases. However, the relative error in the estimate is $1/\sqrt{\phat(x)T\delta x}$, which grows as the event becomes rarer. This is consistent with our claim, that low probabilities come with larger relative errors, and constitutes the key message of this section. Errors in probability estimates behave differently for low probabilities than for high probabilities: absolute errors are smaller for lower probabilities, but relative errors are larger.

Let's assume that the DM is aware of the uncertainties in his estimates and, furthermore, that he does not like surprises. To avoid surprises, he adds the standard error to his estimate of the probability density, $\phat(x)$, in order to construct his decision weight density, $w(x)$. In effect, he constructs a reasonable worst case for each of his estimates. After normalising, this conservative strategy yields generically,
\be
w(x) = \frac{\phat(x)+\err{\phat(x)}}{\int_{-\infty}^{\infty}\left(\phat(s)+\err{\phat(s)}\right)ds}~,
\elabel{weight_density_gen}
\ee
and specifically, for the type of uncertainty we consider,
\be
w(x)= \frac{\phat(x)+\sqrt{\frac{\phat(x)}{T \delta x}}}{\int_{-\infty}^{\infty}\left(\phat(s)+\sqrt{\frac{\phat(s)}{T \delta x}}\right)ds}~.
\elabel{weight_density_spec}
\ee

Note that the cautionary correction term in \eref{weight_density_spec} is parametrised by $T\delta x$, which scales like the number of observations in $[x, x+\delta x]$. As $T\delta x$ grows large, the correction vanishes and both $w(x)$ and $\phat(x)$ become consistent with the asymptotic density, $p(x)$. With perfect information, a DM need not adjust decisions to account for uncertainty.

Does our analysis, culminating in \eref{weight_density_gen} and \eref{weight_density_spec}, reproduce the stylised facts of probability weighting, in particular the inverse-S curve? We check in two ways. First, analytically, by applying the DM's cautionary correction in \eref{weight_density_spec} directly to reference probability density functions. Second, by simulating the DM compiling counts of outcomes drawn from reference distributions, from which he estimates probability densities and their uncertainties. The simulation is meant to explore how noisy the effect is when a DM really only sees a single time series. The Python code is available at \href{https://bit.ly/lml-pw-code-dm-count}{\texttt{bit.ly/lml-pw-code-dm-count}}, and a Jupyter notebook can be loaded to manipulate the code in an online environment at \href{https://bit.ly/lml-pw-dm-count-b}{\texttt{bit.ly/lml-pw-dm-count-b}}. In both cases, we treat the DO as using the reference distribution to make his predictions of the DM's behaviour.

\Fref{square_root_error} shows the resulting PDFs and CDF mappings generated by setting $\phat(x)$ in \eref{weight_density_spec} to be the probability density functions for a Gaussian distribution and a fat-tailed $t$-distribution. Inverse-S curves are found for both distributions and the effect is more pronounced for the fat-tailed distribution.
\begin{figure}[!htb]
\centering
\includegraphics[width=1.0\textwidth]{./figs/square_root_error.pdf}
\caption{\textbf{Mapping PDFs and CDFs with estimation errors.} PDFs (left) and inverse-S curves (right) arising when the DO assumes a Gaussian (scale 1, location 0, top line) or a $t$-distribution (shape 2, location 0, bottom line), and the DM uses decision weights according to \eref{weight_density_spec} with $T\delta x=10$. For the fat-tailed $t$-distribution (in the bottom line) the difference between $p(x)$ and $w(x)$ is more pronounced.}
\flabel{square_root_error}
\end{figure}

\Fref{dm_count_sim} shows the results of a computer simulation of a DM who observes a series of realisations of either Gaussian or $t$-distributed random variables, which he counts into bins. In the simulation, a probability density, $\phat(x)$, is estimated for each bin as $n(x)/T\delta x$ and its uncertainty, $\err{\phat(x)}$, is obtained numerically as standard deviation in each $\phat(x)$ over 1000 parallel simulations. The DM's decision weights are then obtained according to \eref{weight_density_gen}. Again, inverse-S curves are found for both distributions, corroborating our scaling arguments.
\begin{figure}[!htb]
\centering
\includegraphics[width=1.0\textwidth]{./figs/dm_count_sim.pdf}
\caption{\textbf{Simulations of a DM estimating probability densities by counting events in a finite series of observations.} Left: estimated probability densities for $T=100$ Gaussian (top; location 0, scale 2) and $t$-distributed (bottom; location 0, scale 1, shape 1.5) variates counted in bins of width $\delta x=0.4$. Red bars show the estimates, $\phat(x)$, and blue bars show the estimates with one standard error added, $\phat(x)+\err{\phat(x)}$. Right: inverse-S curves for a DO who assumes $p(x)$ follows a Gaussian (top) and $t$-distribution (bottom), while the DM uses decision weight density, $w(x)$, derived by normalising his conservative estimates (blue bars on left) according to \eref{weight_density_gen}.
}
\flabel{dm_count_sim}
\end{figure}

\subsubsection{Typical situations of DO and DM: ergodicity}
To recap: behavioural economists observe that DOs tend to assign lower weights to low-probability events than DMs. While behavioural economists commonly assume that the DM is wrong, we make no such judgement. In any decision problem, the aim of the decision must be taken into account. Crucially, this aim depends on the situation of the individual.

The two types of modellers (DO and DM) pursue different goals. In our thought experiment, the DO is a behavioural scientist without personal exposure to the success or failure of the DM, whom we imagine as a test subject or someone whose behaviour is being observed in the wild. The DM, of course, has such exposure. Throughout the history of economics, it has been a common mistake, by DOs, to assume that DMs optimise what happens to them on average in an ensemble. To the DM, what happens to the ensemble is seldom a primary concern. Instead, he is concerned with what happens to him over time. Not distinguishing between these two perspectives is only permissible if they lead to identical predictions, meaning only if the relevant observables are ergodic \parencite{Peters2019b}.

It is now well known that this is usually not the case in the following sense: DMs are usually observed making choices that affect their wealth, and wealth is usually modelled as a stochastic process that is not ergodic. The ensemble average of wealth does not behave like the time average of wealth.

The most striking example is the universally important case of noisy multiplicative growth, the simplest model of which is geometric Brownian motion, $dx=x(\mu dt+\sigma dW)$. In the present context of human economic decisions, this is the most widely used model of the evolution of invested wealth. The average over the full statistical ensemble (often studied by the DO) of geometric Brownian motion grows as $\exp(\mu t)$. Each individual trajectory, on the other hand, grows in the long run as $\exp[(\mu-\frac{\sigma^2}{2})t]$. If the DO takes the ensemble perspective, he will deem the fluctuations irrelevant whereas, from the DM's time perspective, they reduce growth. So, while a DO curious about the ensemble may suffer no consequences from disregarding rare events, hedging against such events is central to the DM's success.

The difference between how these two perspectives evaluate the effects of probabilistic events is qualitatively in line with the observed phenomena we set out to explain. The DM typically has large uncertainties, especially for low-probability events, and has an evolutionary incentive to err on the side of caution, \ie to behave as though extreme events have a higher probability than in the DO's model.

\section{Fitting the model to experimental results \seclabel{Fitting_the}}
Visually, looking at the figures and the level of noise in the data in \fref{TK1992}, one would conclude that Tversky and Kahneman's physically unmotivated function, $\tilde{F}^{TK}_w(F_p)$ in \eref{correspondence}, fits the data no more efficiently than the functions arising from our mechanistic model. This is particularly evident in the bottom panels of \fref{Gauss_scale_location_both_KT}, which show that a Gaussian, $w(x)$, whose scale and location differ from those of $p(x)$, reproduces the fitted functional shape of $\tilde{F}^{TK}_w(F_p)$.

For completeness and scientific hygiene, in the present section we fit location and scale parameters in the Gaussian and $t$ models for $F_w$ to experimental data from \textcite{TverskyKahneman1992} (depicted in circles in \fref{TK1992}) and from \textcite{TverskyFox1995}. Specifically, in the Gaussian model we fit the location and scale parameters $\mu$ and $\sigma$ in the CDF,
%
\be
F_w\left(x\right) = \Phi\left(\frac{\Phi^{-1}\left(F_p\left(x\right)\right) - \mu}{\sigma}\right)~,
\ee
%
where $\Phi$ is the CDF of the standard normal distribution. In the $t$-model, we fit the location and shape parameters, $\mu$ and $\nu$, in the CDF, $F_w(x)$, of a $t$-distributed random variable (see \secref{Different_shapes}). In both cases, we assume that $F_p(x)$ is that of a standard normal distribution.

In addition to \eref{correspondence} used by Tversky and Kahneman, we fit the function
%
\be
\tilde{F}^{L}_w\left(F_p; \delta,\gamma\right) =\frac{\delta F_p^{\gamma}}{\delta F_p^{\gamma} + \left(1-F_p\right)^{\gamma}}\,,
\elabel{LattimoreFunction}
\ee
%
suggested by \textcite{LattimoreBakerWitte1992} to parametrically describe probability weighting (also used by \textcite{TverskyWakker1995} and \textcite{Prelec1998}). The reason for fitting \eref{LattimoreFunction} is to ensure a fair comparison: the Gaussian and $t$ models are characterised by two parameters, whereas \eref{correspondence} only has one free parameter. \Eref{LattimoreFunction} has two parameters.

\Fref{curvefit} presents the fit results. We obtain very good fits to data for both Gaussian and $t$-distributions, as well as for \eref{correspondence} and \eref{LattimoreFunction}, in the two experiments. It is practically impossible to distinguish between the fitted functions within standard errors. We conclude that our model fits the data well, and unlike \eref{correspondence} or \eref{LattimoreFunction}, the fitted functions are directly derived from a physically plausible mechanism. They are not simply phenomenological.

\begin{figure}[!htb]
\centering
\includegraphics[width=1.0\textwidth]{./figs/curvefit.pdf}
\caption{\textbf{Model fitting to experimental data from \textcite{TverskyKahneman1992} (left) and \textcite{TverskyFox1995} (right).}
Left: \textcite{LattimoreBakerWitte1992} \eref{LattimoreFunction}: $\delta=0.67\,\left(SE = 0.04\right)$, $\gamma=0.58\,\left(\pm0.03\right)$; Gaussian model: $\mu=0.38\,\left(\pm0.06\right)$, $\sigma=1.60\,\left(\pm0.10\right)$; $t$-model: $\nu=1.27\,\left(\pm0.28\right)$, $\mu=0.40\,\left(\pm0.07\right)$; \textcite{TverskyKahneman1992} \eref{correspondence}: $\gamma=0.60\,\left(\pm0.02\right)$. Right: \textcite{LattimoreBakerWitte1992}: $\delta=0.77\,\left(\pm0.01\right)$, $\gamma=0.69\,\left(\pm0.01\right)$; Gaussian model: $\mu=0.22\,\left(\pm0.01\right)$, $\sigma=1.41\,\left(\pm0.03\right)$; $t$-model: $\nu=1.41\,\left(\pm0.21\right)$, $\mu=0.22\,\left(\pm0.03\right)$; \textcite{TverskyKahneman1992}: $\gamma=0.68\,\left(\pm0.01\right)$. Shaded areas indicate two standard errors in the fitted parameter values. The fit was done by implementing the Levenberg-Marquardt algorithm \parencite{Levenberg1944} for non-linear least squares curve fitting.
}
\flabel{curvefit}
\end{figure}

\section{Discussion}

On 28 February 2020, \textcite{Sunstein2020}, a behavioural economist, legal scholar, and former United States Administrator of the Office of Information and Regulatory Affairs, diagnosed that people's concern about a potential coronavirus outbreak in the US was attributable to an extreme case of probability weighting. Supposedly, according to Sunstein, people were neglecting the fact that such an event had a low probability. When the piece was published, many commented that it seemed quite reasonable to them to take precautions, and that Sunstein himself may have underestimated both the severity and likelihood of what lay ahead. One month later, the US suffered a major outbreak of coronavirus.

This sad episode illustrates that an inverted S-curve is a neutral indicator of a difference in opinion. It says nothing about who is right and who is wrong.

The term ``probability weighting'' suggests an obscure mental process, where a DM carries out operations on probabilities. It seems more natural to us to consider a DM modelling events about whose probabilities he is unsure. From this latter point of view, it is easy to think of reasons for a DM's model to differ from a DO's. DMs will often have cause to include additional uncertainty, leading to the frequently observed inverse-S curve.

The model of estimating probabilities from real time series, which we discuss in \secref{Reasons_for}, has qualitative features that display a degree of universality. Relative errors in the DM's probability estimates are always greater for rarer events. A dislike of the unexpected, which explains the systematic overestimation of low probabilities, is similarly common. ``Probability weighting'' is purely descriptive and comes with the ill-conceived connotation of DMs suffering from a cognitive error. The phenomenon is better thought of as DMs making wise decisions given the information available to them. Such information is necessarily limited because, for example, DMs are constrained to collect such information in time.

\newpage
\printbibliography

\appendix
\section{Inverse-S from relative errors in probabilities}
\seclabel{relative_errors}
For an inverse-S curve to emerge, small probability densities have to be overestimated ($w>p$) and large ones underestimated ($w<p$), as is indeed the case, for example in \fref{square_root_error}. Let's connect this statement to one about relative uncertainties. The decision weight is arrived at by adding the probability $p(x)$ to its uncertainty $\err{p(x)}$ and normalising, as we did in \eref{weight_density_gen}, \ie
\be
w(x)=\frac{p(x)+\err{p(x)}}{\int_{-\infty}^{\infty} \left( p(s)+\err{p(s)} \right) ds}~.
\ee
This can be expressed as
\be
w(x)=p(x) \left(\frac{1+\frac{\err{p(x)}}{p(x)}}{\int_{-\infty}^{\infty} p(s)\left\{1+\frac{\err{p(s)}}{p(s)}\right\} ds}\right)~,
\elabel{rel_error}
\ee
where $\frac{\err{p(x)}}{p(x)}$ is the relative error, and the denominator of  \eref{rel_error} is a normalisation constant. If the relative error is larger for small probabilities than for large probabilities, then small probabilities are enhanced more (the summand $\frac{\err{p(x)}}{p(x)}$ in the numerator is greater) than large probabilities. The normalisation constant scales down all probabilities equally, and where the enhancement was greater, $w(x)$ ends up above $p(x)$, and where it was lower $w(x)$ ends up below $p(x)$. So, if the relative error is larger for small probabilities, an inverse-S curve emerges.

We can say one more thing about this procedure. If an inverse-S curve exists, then $p(x)$ and $w(x)$ cross somewhere, see \fref{square_root_error}. This happens when the relative error attains its expectation value (with respect to the density $p$). Rewriting \eref{rel_error} as
\be
w(x)=p(x) \left(\frac{1+\frac{\err{p}}{p}}{1+\ave{\frac{\err{p}}{p}}}\right),
\ee
we see that $w(x)=p(x)$ when $\frac{\err{p}}{p}=\ave{\frac{\err{p}}{p}}.$
\end{document}
