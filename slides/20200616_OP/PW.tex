\begin{frame}
%[allowframebreaks]
{Evolution of Decision Criteria : Ensemble Average Maintenance}{Work in progress}

\begin{align*} \elabel{EvoDTs}
	&	\text{Expectation Value Theory:} &\EV{\D X}	&= \sum_{n=1}^{\infty}
	&	\underbrace{\Dx_n\strut}_{\text{absolute payout}\strut} &\cdot
	&	\underbrace{p_n\strut}_{\text{objective probability}\strut}\\
	%
	\elabel{EvoDTs2}
	&	\text{Expected Utility Theory:} &\EV{\Du\left(X\right)}	&= \sum_{n=1}^{\infty}
	&	\underbrace{\red{\D u\left(x_n\right)}\strut}_{\text{function of Wealth}\strut} 	&	\cdot
	&	\underbrace{p_n\strut}_{\text{objective probability}\strut}  \\
	%
	\elabel{EvoDTs3}
	&	\text{Cumulative Prospect Theory:} &\mathfrak{V}\left( f_X \right)
	&=	\sum_{n=1}^{\infty} & \underbrace{\red{\mathfrak{v}\left(\Dx_n\right)}\strut}_{\text{function of the payout}\strut} &	\cdot &	\underbrace{\blue{\pi\left(p_n\right)}\strut}_{\text{function of obj. probability}\strut}
\end{align*}

% \vfill

\gray{\footnotesize $x$ wealth, $\Dx$ payout, $p$ probability, $u$ utility function, $\mathfrak{V}$ value/prospect,\\
$\mathfrak{v}$ value function, $\pi$ probability weighting, $f_X$ lottery/probability mass function of a random variable}

\label{EvolutionDT}

% \framebreak
% 
% \begin{align*}
%  \elabel{EvoDTsEDU}
% 	&	\text{Expected discounted utility:} &\EA{\beta^t\Du\left(X\right)}	&= \sum_{n=1}^{\infty}
% 	&	\underbrace{\blue{\beta^t \D u\left(x_n\right)}\strut}_{\text{function of Wealth}\strut} 	&	\cdot
% 	&	\underbrace{p_n\strut}_{\text{objective probability}\strut}  \\
% 	%
% 	\elabel{EvoDTsBUT}
% 	&	\text{Behavioural Utility:} &\mathfrak{V}\left( f_x \right)
% 	&=	\sum_{n=1}^{\infty} & \underbrace{\blue{\mathfrak{v}\left(\Dx_n\right)}\strut}_{\text{function of the payout}\strut} &	\cdot &	\underbrace{\blue{\pi\left(p_n\right)}\strut}_{\text{function of obj. probability}\strut}
% \end{align*}

\end{frame}


% \section[Probability Weighting]{What are weighting for? A mechanistic explanation of probability weighting}
\section{Main Results}

\begin{frame}{Main results}
\begin{center}
\only<1->{
	\includegraphics[width=.45\textwidth]{../../figs/KT_result_nolegend.pdf}
	}
\uncover<2>{
\includegraphics[width=.45\textwidth]{../../figs/Our_result_nolegend.pdf}
	}
\end{center}

% \begin{enumerate}
% 	\item	inverse-S shape can be explained by difference in uncertainty
% 	\item cautious estimation of probabilities generates such differences
% % 	\item	relative estimation error in $p(x)$ is greater for rarer events 	
% \end{enumerate}
\label{MainResults}
% \hyperlink{weight_vs_estimate}{\beamergotobutton{PW K\&T 1979}}
\end{frame}

\section{Probability Weighting}

\begin{frame}{Definition of Probability Weighting (PW)}

\begin{columns}%[T]
\column{0.5\textwidth}
{\centering	\includegraphics[width=.9\textwidth]{../../figs/TK1992.pdf} }
\parencite[p. 310, Fig. 1,relabelled axes]{TverskyKahneman1992}

\column{0.5\textwidth}
\begin{itemize}
%   \item $ w(x) = f\left(F_p(x)\right)$ \parencite{Quiggin1982}
%   \item low probabilities treated as higher;\\ 
%   high probabilities treated as lower
  \item empirical pattern: inverse-S shape
  \item Cumulative Prospect Theory (CPT) \nocite{}
\end{itemize}
\vspace{1em}
\red{Classical interpretation of PW:}
\bi
	\item	\red{maladaptive irrational cognitive bias}
\ei
\vspace{1em}
\begin{block}{In search of a mechanism}
	\begin{itemize}
	  \item[$\hookrightarrow$] How does this pattern emerge?
  	\item[$\hookrightarrow$] Can we derive a functional form\\ 
	(rather than fit a function)?
	\end{itemize}
\end{block}

\end{columns}
\end{frame}

\section{Setup}
\begin{frame}{Setup}
\begin{center}
Task: model payout, $x$, of a gamble as a random variable.
\end{center}
\begin{columns}[T]
\column{0.5\textwidth}
	\centering
	\textbf{Disinterested Observer (DO)} \\
	\vspace{0.5em}
	\includegraphics[height=3cm]{img/TverskyKahnemanFunny} \\
	\vspace{0.5em}
\column{0.5\textwidth}
	\centering
	\textbf{Decision Maker (DM)} \\
	\vspace{0.5em}
	\includegraphics[height=3cm]{img/LabRat} \\
	\vspace{0.5em}
\end{columns}

\begin{columns}[T]
\column{0.05\textwidth}
\column{0.35\textwidth}
	\vspace{1em}
% 	\centering
	DO assigns \red{PDF $p(x)$} \\
  $\hookrightarrow$ \red{CDF $F_p(x)$}\\
\column{0.2\textwidth}
% \centering
% 	\includegraphics[height=3cm]{img/coinflip}
\column{0.4\textwidth}
	\vspace{1em}
% 	\centering
  DM assigns different \blue{PDF $w(x)$}\\
  $\hookrightarrow$ \blue{CDF $F_w(x)$}\\
\end{columns}
\end{frame}

% \section{Location and Scale of PDFs}
% \hyperlink{t-distribution}{\beamergotobutton{HTD}}
% Transmission of different uncertainties from PDFs into CDFs}

\begin{frame}{Scales, Locations, Shapes}
\centering
	\includegraphics[width=0.78\textwidth]{../../figs/2GaussianPDFs2Scales2Locations.pdf}
% 	\includegraphics[width=0.35\textwidth]{../../figs/pdfs_diff_scale_and_loc.pdf}
	\includegraphics[width=0.42\textwidth]{../../figs/diff_shapes_Gauss_t.pdf}
\end{frame}

\begin{frame}{Thought Experiment: DM assumes greater scale}
\centering
	\includegraphics[width=.35\textwidth]{../../figs/2GaussianPDFs2Scales.pdf} \\
% 2. Corresponding CDFs\\
% \only<2>{
	\includegraphics[width=.8\textwidth]{../../figs/mapping_cdfs_noarrows.pdf} \\
% 	}
% 3. Add arrows to CDFs\\
% \only<3>{
% 	\includegraphics[width=0.8\textwidth]{../../figs/mapping_cdfs_1arrow.pdf} \\
% 	}
% 4. Explain how to transform to get to inverse S (add label to red line)\\
% \only<4>{
% 	\includegraphics[width=0.8\textwidth]{../../figs/mapping_cdfs_2arrows.pdf} \\
% 	}
% \only<5>{
% 	\includegraphics[width=0.8\textwidth]{../../figs/mapping_cdfs_3arrows.pdf} \\
% 	}
% \only<6>{
% 	\includegraphics[width=0.8\textwidth]{../../figs/mapping_cdfs_4arrows.pdf} \\
% 	}
% \only<7>{
% 	\includegraphics[width=0.8\textwidth]{../../figs/mapping_cdfs_5arrows.pdf} \\
% 	}
% \only<8>{
% 	\includegraphics[width=0.8\textwidth]{../../figs/mapping_cdfs_6arrows.pdf} \\
% 	}
% \only<9>{
% 	\includegraphics[width=0.8\textwidth]{../../figs/mapping_cdfs_7arrows.pdf} \\
% 	}
% \only<10>{
% 	\includegraphics[width=0.8\textwidth]{../../figs/mapping_cdfs.pdf} \\
% 	}
\end{frame}

\section{Functional Form}
\begin{frame}{Functional form of the weighting function}
Gaussian case with different scale:
\begin{equation}
	w(p) =
% 	\underbrace{
	p^{\frac{1}{\alpha^2}}
% 	\vphantom{\frac{\left(\sigma^2\right)^{\frac{\alpha^2}{\alpha^2}}}{\alpha}
% 	}
% 	}_{\text{power law}}
	\underbrace{
	\frac{\left(2\pi\sigma^2\right)^{\frac{1-\alpha^2}{2\alpha^2}}}{\alpha\strut}
	}_{\text{normalisation factor}}
	~,
\end{equation}
where
\begin{itemize}
  \item DO's scale is $\sigma$
  \item DM's scale is $\alpha\sigma$
%   \item $\alpha < 1 \to$ S shape
%   \item $\alpha > 1 \to	$ inverse-S shape
\end{itemize}
\end{frame}

% \begin{frame}
% Numerically easy for any pair of distributions (models):
% \begin{enumerate}
% 	\item list values of DO's CDF, \red{$F_p(x)$}, at set ${x_i}$
% 	\item list values of DM's CDF, \blue{$F_w(x)$}, at same ${x_i}$
% 	\item plot \blue{$F_w(x)$} \vs \red{$F_p(x)$}
% % 	\item map DM's CDF onto DO's CDF, \ie \blue{$F_w(F_p)$}
% \end{enumerate}
% % XXX illustrate with corresponding lists and figure
% \end{frame}

% \begin{frame}{Asymmetry from different locations}
% \centering
% 	\includegraphics[width=0.75\textwidth]{../../figs/Gauss_scale_location_both_KT.pdf}
% 	\label{LocationScale}
% \end{frame}


\begin{frame}{Interim conclusion}
\label{InterimConclusion}
\centering \includegraphics[width=0.9\textwidth]{../../figs/KT_vs_Our_result_LocScale.pdf}

% \vspace{2em}
\bi
	\item DM's greater scale gives inverse-S shape (unimodal distributions)
	\item difference in locations gives asymmetry
	\item reproduces observations of probability weighting
	\item[]
	\item[] \textit{Job done. Thank you for your attention ;)}
% 	\hfill
% 	\hyperlink{FunctionalForms}{\beamergotobutton{Functional Forms}}
\ei
\end{frame}

\section{Ergodicity Question}

\begin{frame}{The Ergodicity Question}
\begin{columns}[T]
\column{.35\textwidth}
	\bc \textbf{Typical DO concern} \ec
	What happens on average to the \red{ensemble} of subjects?
\column{.02\textwidth}
\centering \vspace{4em}  \red{\large $\neq$}
\column{.35\textwidth}
	\bc \textbf{Typical DM concern} \ec
	What happens to me \\
	\blue{on average over time}?
\end{columns}
\end{frame}


\section{Estimation}

\begin{frame}{Why DM's greater scale?}
%DM's adaptive rationality: err on the side of caution:
\begin{itemize}
  \item DM has no control over experiment
  \item experiment may be unclear to DM
  \item DM may not trust DO
%	\item uncertain outcome is consequential only to the DM,
% 	\item DM's ignorance,
  \item \ldots
\end{itemize}
\end{frame}

\begin{frame}{Experiencing probabilities}

\begin{itemize}
%  \item ``probability'' is polysemous \quelle{\parencite{Gigerenzer1991,Gigerenzer2018,HertwigGigerenzer1999}}
  \item probabilities are not observable
  \item probabilities encountered as
  	\begin{itemize}
  		\item known frequencies in ensemble of experiments (DO)
  		\item frequencies estimated over time (DM)
  	\end{itemize}
  \item[$\hookrightarrow$] \textbf{estimates have uncertainties -- cautious DM accounts for these}
%  \item typical situation: rarer event $\longrightarrow$ larger relative error estimated probability

\end{itemize}
\end{frame}


\begin{frame}{Estimating probabilities}
\begin{columns}[T]
\column{0.5\textwidth}
\textbf{Rare Event} %\hfill \includegraphics[height=1.5cm]{img/BlackSwan} \hfill
\begin{itemize}
  \item $p(x) = 0.001$
  \item \num{100} observations
  \item $\sim 99.5\%$ get 0 or 1 events
  \item $\phat(x) = 0$ or $\phat(x)=0.01$
  \item[$\hookrightarrow$] $\phat(x)$ off by \num{1000}\% 
\end{itemize}
\column{0.5\textwidth}
\textbf{Common Event} %\hfill \includegraphics[height=1.5cm]{img/WhiteSwan}  \hfill
\begin{itemize}
  \item $p(x)=0.5$
  \item \num{100} observations
  \item $\sim 99.5\%$ get between 35 and 65 events,
  \item $0.35< \phat(x) <0.65$
  \item[$\hookrightarrow$] $\phat(x)$ off by 30\%
\end{itemize}
\end{columns}
\vspace{2em}
 \centering
$\hookrightarrow$ small $p(x)$, small count $\rightarrow$ big uncertainty
\end{frame}


% \begin{frame}{Relative estimation error is large for rare events}
% \begin{center}
% \begin{table}[!htb]
%   \begin{tabular}{@{}ccccc@{}}
% \toprule[2pt]
% \makecell{Asymptotic\\probability} & \makecell{Most likely\\count} & \makecell{Standard error\\in count} & \makecell{Standard error\\in probability} & \makecell{Relative error\\in probability}\\
% \midrule[2pt]
% % .5 &	5000 & 71	& 0.01 & 0.71\%\\
% 0.1 & 1000 & 32 & 0.003 & 3\%\\
% 0.01 & 100 & 10 & 0.001 & 10\%\\
% 0.001 & 10 & 3 & 0.0003& 30\%\\
% 0.0001 & 1 & 1 & 0.0001 &100\%\\
% \bottomrule[2pt]
% \end{tabular}
% \caption{$T = \num{10000}$, assuming Poisson statistics, relative estimation errors $\sim \nicefrac{1}{\sqrt{\text{count}}}$}
% \label{errors}
% \end{table}
% \vspace{2em}
% $\hookrightarrow$ small $p(x)$, small count \\
% $\hookrightarrow$ small count, big uncertainty
% \end{center}
% \end{frame}

\begin{frame}{DMs don't like surprises}

To avoid surprises, DMs \blue{add estimation uncertainty $\err{\phat(x)}$} to every estimated probability, then normalize, s.t.
\begin{equation}
	w(x)=\frac{\phat(x)\blue{+\err{\phat(x)}}}{\int \left(\phat(s) \blue{+\err{\phat(s)}} \right)ds}
\end{equation}
\pause
% \ldots visually similar to function chosen by Kahneman and Tversky.
\centering
	\includegraphics[width=.9\textwidth]{../../figs/square_root_error_1Gaussian.pdf} \\
\end{frame}


%\begin{frame}{Estimation of the decision weights}
%
%	Using the count $n(x)$ to form the best estimate and add to it the uncertainty about best estimate 
%	\begin{align}
%		w(x)						&\approx	\frac{n(x)}{T\delta x} \pm \frac{\sqrt{n(x)}}{T \delta x} \\
%		w(x)						&\approx	\phat(x) \pm \err{\phat(x)} 	\elabel{prob_est}
%	\end{align}
%	with the standard error expressed in terms of the estimate itself
%	\begin{align}
%		\err{\phat(x)}	&\equiv		\frac{\sqrt{n(x)}}{T \delta x} = \sqrt{\frac{\phat(x)}{T \delta x}} \\
%		\lim_{T\to\infty} w(x)	&\to p(x)					
%	\end{align}
%
%\end{frame}
%
%\begin{frame}
%\begin{center}
%  \includegraphics[width=.8\textwidth]{../../figs/dm_count_sim}
%\end{center}
%\end{frame}
%
%
%\end{frame}

% \begin{frame}{Simulation of the Estimation}
% \begin{center}
% 	\includegraphics[width=.65\textwidth]{img/dm_count_sim} \\
% 	$T = 100$, estimates of \red{$\phat(x)$} in red, estimates with one standard error \blue{$\phat(x) + \err{\phat(x)}$} in blue 
% \end{center}
% 
% \end{frame}
% 
% \begin{frame}
% 	Using the fact that $n(x)$ is a random variable itself, $n(x) \sim Poisson$, its fluctuations scale like $\sqrt{n(x)}$ \\
% 
% 	Using the count $n(x)$ to infer the asymptotic PDF as
% 	\begin{align}	  
% 		p(x)	&\approx \frac{n(x)}{T\delta x} \pm \frac{\sqrt{n(x)}}{T \delta x} \\
% 					&\approx \phat(x) \pm \err{\phat(x)}
% 	\elabel{prob_est}
% 	\end{align}
% 	with the standard error (expressed in terms of the estimate itself)
% 	$$\err{\phat(x)} \equiv \frac{\sqrt{n(x)}}{T \delta x} = \sqrt{\frac{\phat(x)}{T \delta x}}$$
% 	\bi
% 		\item standard error $\err{\phat(x)}$ shrinks as the probability decreases
% 		\item relative error in the estimate is $1/\sqrt{\phat(x)T\delta x}$ grows as the event becomes rarer
% 		\item consistent with our claim, that low probabilities come with larger relative errors
% 		\item[$\hookrightarrow$] Errors in probability estimates behave differently for low probabilities than for high probabilities: absolute errors are smaller for lower probabilities, but relative errors are larger
% 	\ei
% \end{frame}

\section{Conclusion}




\begin{frame}{Conclusion}
\begin{columns}[T]
\column{.5\textwidth}
Classical interpretation of PW
\bi
  \item overestimation of low probability events
  \item underestimation of high probability events
	\item[$\hookrightarrow$]	maladaptive irrational cognitive bias
\ei
\column{.5\textwidth}
Ergodicity Economics and PW
\begin{itemize}
  \item inverse-S shape: neutral indicator of different models of the world
%   \item relative probability estimation errors are greater for rarer events
	\item reported observations consistent with DM's extra uncertainty
	\item may arise from DM estimating probabilities over time
  \item[$\hookrightarrow$] Probability weighting is rational cautious behaviour under uncertainty over time
%   \item $\uparrow$ DM's uncertainty $\to$ inverse-S $\uparrow$
%   \\ \fullcite{PetersETAL2020R1}
% 	\item See blog post for the basic reasoning \fullcite{Peters2020}
% 	\item See blog post for the basic reasoning \fullcite{Buchanan2020}
\end{itemize}
\end{columns}
\vfill
\begin{itemize}
  \item testable prediction $\to$ Let's run an experiment!
  \item Manuscript at \url{https://bit.ly/lml-pw-r1}
  \item Interactive code at \url{https://bit.ly/lml-pw-count-b}    
\end{itemize}
% \vspace{1em}
% \begin{center}
% 	\includegraphics[width=.8\textwidth]{../../figs/Our_result_LocScale_vs_KT.pdf}
% \end{center}

\pause
\centering
\vfill
{\Large \textsc{\textbf{Thank you for your attention!}}}

\end{frame}
